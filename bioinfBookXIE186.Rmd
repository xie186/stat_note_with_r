---
title: "Notes of R for Bioinformatics"
author: "Shaojun Xie"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook: default
  bookdown::pdf_book: default
bibliography: ["reference.bib"]

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=6, fig.height=6) 

```

# Preface {-}

This book includes the notes I have during the use of R as a Bioinformatician. 

```{r echo=FALSE}
# Support me {-}

#<a href="https://www.buymeacoffee.com/omics"  target="_blank"><img src="https://user-images.githubusercontent.com/20909751/53546583-c30aa300-3afa-11e9-8ba3-1ca25ac863c4.png" title="buymecoffee" alt="coffee"></a>

```


```{r echo=FALSE}

# https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them
list.of.packages <- c("bookdown", "combinat",  "LaF", "igraph", "network", "sna", "visNetwork", "threejs", "ndtv", "RColorBrewer", "png", "networkD3", "animation", "maps", "geosphere", "RColorBrewer", "png", "animation", "visNetwork", "threejs", "networkD3", "ndtv", "maps", "geosphere", "psych", "UsingR", "bnlearn", "", "bnlearn", "mice", "VIM", "dplyr", "corrplot", "FactoMineR", "FactoInvestigate", "PerformanceAnalytics", "AppliedPredictiveModeling", "caret", "ellipse", "", "combinat", "RISmed", "manipulate", "lme4")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dependencies = T)

source("https://bioconductor.org/biocLite.R")
list.of.bioconductorpakg <- c("ropls")
new.packages <- list.of.bioconductorpakg[!(list.of.bioconductorpakg %in% installed.packages()[,"Package"])]

BiocInstaller::biocLite(new.packages, suppressUpdates=TRUE)
```

<!--chapter:end:index.Rmd-->

# (PART) Introduction {-}

# Introduction of R

I started to use R to generate figures for the project data. For example, I developed [`ViewBS`](https://github.com/xie186/ViewBS) to visualize whole genome bisulfite sequencing data. In ViewBS, I use R






Learning objectivesLearning Objectives

✓ Become comfortable with RStudio (a graphical interface for R)

✓ Fluently interact with R using RStudio

✓ Become familiar with R syntax

✓ Understand data structures in R

✓ Inspect and manipulate data structures

✓ Install packages and use functions in R 

✓ Visualize data using simple and complex plotting methods

## Setup



## Basic systax


### Install packages

### Install 

#### Getting help on functions and packages


## Data type

## Data structures

R has multiple data structures. If you are familiar with excel you can think of data structures as building blocks of a table and the table itself, and a table is similar to a sheet in excel. Most of the time you will deal with tabular data sets, you will manipulate them, take sub-sections of them. It is essential to know what are the common data structures in R and how they can be used. R deals with named data structures, this means you can give names to data structures and manipulate or operate on them using those names.

## Read from and write into files


### Read Random Rows from A Huge File

>> Given R data frames stored in the memory, sometimes it is beneficial to sample and examine the data in a large-size csv file before importing into the data frame. To the best of my knowledge, there is no off-shelf R function performing such data sampling with a relatively low computing cost. Therefore, I drafted two utility functions serving this particular purpose, one with the LaF library and the other with the reticulate library by leveraging the power of Python. While the first function is more efficient and samples 3 records out of 336,776 in about 100 milliseconds, the second one is more for fun and a showcase of the reticulate package.

```{r}
#install.packages("LaF")
library(LaF)
sample1 <- function(file, n) {
  lf <- laf_open(detect_dm_csv(file, sep = ",", header = TRUE, factor_fraction = -1))
  return(read_lines(lf, sample(1:nrow(lf), n)))
}

sample1("data/galton.csv", 3)
```


### Reference

Read Random Rows from A Huge CSV File: https://www.r-bloggers.com/read-random-rows-from-a-huge-csv-file/



## Functions


## Control structure



## Reference

Introduction to R: https://wiki.harvard.edu/confluence/display/hbctraining/Introduction+to+R


R graphics with ggplot2 workshop notes: http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html






<!--chapter:end:00_R_intro.Rmd-->

# Plot in R 

## Read the data

Consider the data collected by Francis Galton in the 1880s, stored in a modern format in the `galton.csv` file. In this file, `heights` is the variable containing the child’s heights, while the `father`’s and `mother`’s height is contained in the variables father and mother.
The family variable is a numerical code identifying children in the same family; the number of kids in this family is in `nkids`.


```{r}
## Data from  https://github.com/thomas-haslwanter/statsintro_python/blob/master/ISP/Code_Quantlets/08_TestsMeanValues/anovaOneway/galton.csv
tab<-read.csv("data/galton.csv")
head(tab)
```



## Line plot

## Basic line plot

```{r}
plot(tab$father, tab$height)
```


## Color the dots based on the `sex` information

```{r}
col_info = ifelse(tab$sex=="M", "royalblue", "darkred")
plot(tab$father, tab$height, col=col_info)
```


```{r}
col_info = ifelse(tab$sex=="M", "royalblue", "darkred")
plot(tab$father, tab$height, col=col_info, xlim=c(60, 80), ylim=c(60, 80))
legend(76, 65,pch=c(19,19),col=c("royalblue", "darkred"),c("female", "male"),bty="o",cex=.8)

```



## 







```{r}
meth_lev=read.table("data/global_meth_lev.tsv", header=TRUE)
meth_lev
```



## Draw a circle in R 


```{r}
# initialize a plot
plot(c(-1, 1), c(-1, 1), type = "n")

# prepare "circle data"
radius <- 1
theta <- seq(0, 2 * pi, length = 200)

# draw the circle
lines(x = radius * cos(theta), y = radius * sin(theta))
```



## Network 


### 

```

##========================================================##
##                                                        ##
##   Network Visualization with R                         ##
##   Polnet 2018 Workshop, Washington, DC                 ##
##   www.kateto.net/polnet2018                            ##
##                                                        ##
##   Katherine (Katya) Ognyanova                          ##
##   Web: kateto.net | Email: katya@ognyanova.net         ##
##   GitHub: kateto  | Twitter: @Ognyanova                ##
##                                                        ##
##========================================================##



# ================ Introduction ================ 


# Download handouts and example data: bit.ly/polnet2018
# Online tutorial: kateto.net/polnet2018


# CONTENTS
#
#   1. Working with colors in R plots
#   2. Reading in the network data
#   3. Network plots in 'igraph'
#   4. Plotting two-mode networks
#   5. Plotting multiplex networks
#   6. Quick example using 'network' 
#   7. Simple plot animations in R
#   8. Interactive JavaScript networks 
#   9. Interactive and dynamic networks with ndtv-d3
#  10. Plotting networks on a geographic map


# KEY PACKAGES
# Install those now if you do not have the latest versions. 
# (please do NOT load them yet!)

install.packages("igraph") 
install.packages("network") 
install.packages("sna")
install.packages("visNetwork")
install.packages("threejs")
install.packages("ndtv")


# OPTIONAL PACKAGES
# Install those if you would like to run through all of the
# examples below (those are not critical and can be skipped).

install.packages("RColorBrewer") 
install.packages("png")
install.packages("networkD3")
install.packages("animation")
install.packages("maps")
install.packages("geosphere")
 



# ================ 1. Colors in R plots ================ 
 

#  -------~~ Colors --------


# In most R functions, you can use named colors, hex, or rgb values:

plot(x=1:10, y=rep(5,10), pch=19, cex=5, col="dark red")
points(x=1:10, y=rep(6, 10), pch=19, cex=5, col="#557799")
points(x=1:10, y=rep(4, 10), pch=19, cex=5, col=rgb(.25, .5, .3))

# In the simple base plot chart above, x and y are point coordinates, 'pch' 
# is the point symbol shape, 'cex' is the point size, and 'col' is the color.

# To see the parameters for plotting in base R, check out ?par
# Some key ones include "bg" for background color, 'cex' for scaling text size,
# 'mar' for plot margins, 'new' for plotting witout clearing the previous plot,
# 'mfcol' & 'mfrow' split plotting space into rows and columns for multiple plots
?par

# To close the current graphics device clearing the plot and plot parameters, use: 
dev.off()

# If you plan on using the built-in color names, here's what they are: 
colors() # all colors
grep("blue", colors(), value=T) # colors that have 'blue' in the name

# You may notice that rgb here ranges from 0 to 1. While this is the R default,
# you can also set it for the more typical 0-255 range: 
rgb(10, 100, 100, maxColorValue=255) 


#  -------~~ Transparency --------

# We can also set the opacity/transparency using the parameter 'alpha' (range 0-1):
plot(x=1:5, y=rep(5,5), pch=19, cex=16, col=rgb(.25, .5, .3, alpha=.5), xlim=c(0,6))  

# If we have a hex color representation, we can set the transparency alpha 
# using 'adjustcolor' from package 'grDevices'. For fun, let's also set the
# the plot background to black using the par() function for graphical parameters.
# We could also set the margins in par() with mar=c(bottom, left, top, right).
par(bg="black")

col.tr <- grDevices::adjustcolor("#557799", alpha=0.7)
plot(x=1:5, y=rep(5,5), pch=19, cex=20, col=col.tr, xlim=c(0,6)) 

dev.off()


#  -------~~ Palettes --------

# In many cases, we need a number of contrasting colors, or multiple shades of a color.
# R comes with some predefined palette function that can generate those for us.
pal1 <- heat.colors(5, alpha=1)   # generate 5 colors from the heat palette, opaque
pal2 <- rainbow(5, alpha=.5)      # generate 5 colors from the heat palette, semi-transparent
plot(x=1:10, y=1:10, pch=19, cex=10, col=pal1)
par(new=TRUE) # tells R not to clear the first plot before adding the second one
plot(x=10:1, y=1:10, pch=19, cex=10, col=pal2)

# We can also generate our own gradients using colorRampPalette().
# Note that colorRampPalette returns a FUNCTION that we can use 
# to generate as many colors from that palette as we need.

palf <- colorRampPalette(c("gray70", "dark red", "orange")) 
plot(x=10:1, y=1:10, pch=19, cex=10, col=palf(10)) 

# To add transparency to colorRampPalette, you need to add a parameter `alpha=TRUE`:
palf <- colorRampPalette(c(rgb(1,1,1, .2),rgb(.8,0,0, .7)), alpha=TRUE)
plot(x=10:1, y=1:10, pch=19, cex=10, col=palf(10)) 


#  -------~~ ColorBrewer --------

# Finding good color combinations is a tough task - and the built-in R palettes
# are rather limited. Thankfully there are other available packages for this:

# install.packages("RColorBrewer")
library("RColorBrewer")

display.brewer.all()

# This package has one main function, called 'brewer.pal'.
# Using it, you just need to select the desired palette and a number of colors.
# Let's take a look at some of the RColorBrewer palettes:
display.brewer.pal(8, "Set3")
display.brewer.pal(8, "Spectral")
display.brewer.pal(8, "Blues")


# Plot figures using ColorBrewer
# We'll use par() to plot multiple figures.
# plot row by row: mfrow=c(number of rows, number of columns)
# plot column by column: mfcol=c(number of rows, number of columns)

par(mfrow=c(1,2)) # plot two figures - 1 row, 2 columns

pal3 <- brewer.pal(10, "Set3")
plot(x=10:1, y=10:1, pch=19, cex=6, col=pal3)
plot(x=10:1, y=10:1, pch=19, cex=6, col=rev(pal3)) # backwards

dev.off() # shut off the  graphic device to clear the two-figure configuration.


detach("package:RColorBrewer")



# ================ 2. Reading network data into 'igraph' ================


# Download an archive with the data files from http://bit.ly/polnet2018  

# Clear your workspace by removing all objects returned by ls():
rm(list = ls()) 
 
# Set the working directory to the folder containing the workshop files:
setwd("C:/polnet2018") 

# If you don't know the path to the folder and you're in RStudio, go to the
# "Session" menu -> "Set Working Directory" -> "To Source File Location"


library("igraph")


# -------~~ DATASET 1: edgelist  --------
 
# Read in the data:
nodes <- read.csv("./Data files/Dataset1-Media-Example-NODES.csv", header=T, as.is=T)
links <- read.csv("./Data files/Dataset1-Media-Example-EDGES.csv", header=T, as.is=T)

# Examine the data:
head(nodes)
head(links)

# Converting the data to an igraph object:
# The graph_from_data_frame() function takes two data frames: 'd' and 'vertices'.
# 'd' describes the edges of the network - it should start with two columns 
# containing the source and target node IDs for each network tie.
# 'vertices' should start with a column of node IDs. It can be omitted.
# Any additional columns in either data frame are interpreted as attributes.

net <- graph_from_data_frame(d=links, vertices=nodes, directed=T) 

# Examine the resulting object:
class(net)
net 

# The description of an igraph object starts with four letters:
#     D or U, for a directed or undirected graph
#     N for a named graph (where nodes have a `name` attribute)
#     W for a weighted graph (where edges have a `weight` attribute)
#     B for a bipartite (two-mode) graph (where nodes have a `type` attribute)
# The two numbers that follow (17 49) refer to the number of nodes and edges in the graph.
# The description also lists node & edge attributes


# We can access the nodes, edges, and their attributes:
E(net)
V(net)
E(net)$type
V(net)$media

# Or find specific nodes and edges by attribute:
# (that returns objects of type vertex sequence / edge sequence)
V(net)[media=="BBC"]
E(net)[type=="mention"]


# If you need them, you can extract an edge list 
# or a matrix back from the igraph networks.
as_edgelist(net, names=T)
as_adjacency_matrix(net, attr="weight")

# Or data frames describing nodes and edges:
as_data_frame(net, what="edges")
as_data_frame(net, what="vertices")


# You can also access the network matrix directly:
net[1,]
net[5,7]

# First attempt to plot the graph:
plot(net) # not pretty!

# Removing loops from the graph:
net <- simplify(net, remove.multiple = F, remove.loops = T) 

# Let's and reduce the arrow size and remove the labels:
plot(net, edge.arrow.size=.4,vertex.label=NA)
 

# -------~~ DATASET 2: matrix  --------

 
# Read in the data:
nodes2 <- read.csv("./Data files/Dataset2-Media-User-Example-NODES.csv", header=T, as.is=T)
links2 <- read.csv("./Data files/Dataset2-Media-User-Example-EDGES.csv", header=T, row.names=1)

# Examine the data:
head(nodes2)
head(links2)

# links2 is a matrix for a two-mode network:
links2 <- as.matrix(links2)
dim(links2)
dim(nodes2)

# Create an igraph network object from the two-mode matrix: 
net2 <- graph_from_incidence_matrix(links2)

# To transform a one-mode network matrix into an igraph object,
# we would use graph_from_adjacency_matrix()


# A built-in vertex attribute 'type' shows which mode vertices belong to.
table(V(net2)$type)

plot(net2,vertex.label=NA)

# Examine the resulting object:
class(net2)
net2 




# ================ 3. Network plots in 'igraph' ================
 

#  ------~~ Plot parameters in igraph --------

# Check out the node options (starting with 'vertex.') 
# and the edge options # (starting with 'edge.'). 
# A list of options is also included in your handout.
?igraph.plotting

# We can set the node & edge options in two ways - one is to specify
# them in the plot() function, as we are doing below.

# Plot with curved edges (edge.curved=.1) and reduce arrow size:
plot(net, edge.arrow.size=.4, edge.curved=.1)

# Set node color to orange and the border color to hex #555555
# Replace the vertex label with the node names stored in "media"
plot(net, edge.arrow.size=.2, edge.curved=0,
     vertex.color="orange", vertex.frame.color="#555555",
     vertex.label=V(net)$media, vertex.label.color="black",
     vertex.label.cex=.7) 

# The second way to set attributes is to add them to the igraph object.

# Generate colors based on media type:
colrs <- c("gray50", "tomato", "gold")
V(net)$color <- colrs[V(net)$media.type]


# Compute node degrees (#links) and use that to set node size:
deg <- degree(net, mode="all")
V(net)$size <- deg*3
# Alternatively, we can set node size based on audience size:
V(net)$size <- V(net)$audience.size*0.7

# The labels are currently node IDs.
# Setting them to NA will render no labels:
V(net)$label.color <- "black"
V(net)$label <- NA

# Set edge width based on weight:
E(net)$width <- E(net)$weight/6

#change arrow size and edge color:
E(net)$arrow.size <- .2
E(net)$edge.color <- "gray80"

# We can even set the network layout:
graph_attr(net, "layout") <- layout_with_lgl
plot(net) 

# We can also override the attributes explicitly in the plot:
plot(net, edge.color="orange", vertex.color="gray50") 


# We can also add a legend explaining the meaning of the colors we used:
plot(net) 
legend(x=-1.1, y=-1.1, c("Newspaper","Television", "Online News"), pch=21,
       col="#777777", pt.bg=colrs, pt.cex=2.5, bty="n", ncol=1)


# Sometimes, especially with semantic networks, we may be interested in 
# plotting only the labels of the nodes:

plot(net, vertex.shape="none", vertex.label=V(net)$media, 
     vertex.label.font=2, vertex.label.color="gray40",
     vertex.label.cex=.7, edge.color="gray85")


# Let's color the edges of the graph based on their source node color.
# We'll get the starting node for each edge with "ends()".
edge.start <- ends(net, es=E(net), names=F)[,1]
edge.col <- V(net)$color[edge.start]

plot(net, edge.color=edge.col, edge.curved=.4)



#  -------~~ Network Layouts in 'igraph' --------


# Network layouts are algorithms that return coordinates for each
# node in a network.

# Let's generate a slightly larger 100-node graph using 
# a preferential attachment model (Barabasi-Albert).

net.bg <- sample_pa(100, 1.2) 
V(net.bg)$size <- 8
V(net.bg)$frame.color <- "white"
V(net.bg)$color <- "orange"
V(net.bg)$label <- "" 
E(net.bg)$arrow.mode <- 0
plot(net.bg)

# Now let's plot this network using the layouts available in igraph.

# You can set the layout in the plot function:
plot(net.bg, layout=layout_randomly)

# Or calculate the vertex coordinates in advance:
l <- layout_in_circle(net.bg)
plot(net.bg, layout=l)

# l is simply a matrix of x,y coordinates (N x 2) for the N nodes in the graph. 
# You can generate your own:
l
l <- cbind(1:vcount(net.bg), c(1, vcount(net.bg):2))
plot(net.bg, layout=l)

# This layout is just an example and not very helpful - thankfully
# 'igraph' has a number of built-in layouts, including:

# Randomly placed vertices
l <- layout_randomly(net.bg)
plot(net.bg, layout=l)

# Circle layout
l <- layout_in_circle(net.bg)
plot(net.bg, layout=l)

# 3D sphere layout
l <- layout_on_sphere(net.bg)
plot(net.bg, layout=l)

# The Fruchterman-Reingold force-directed algorithm 
# Nice but slow, most often used in graphs smaller than ~1000 vertices. 
l <- layout_with_fr(net.bg)
plot(net.bg, layout=l)

# You will also notice that the F-R layout is not deterministic - different 
# runs will result in slightly different configurations. Saving the layout 
# in l allows us to get the exact same result multiple times.
par(mfrow=c(2,2), mar=c(1,1,1,1))
plot(net.bg, layout=layout_with_fr)
plot(net.bg, layout=layout_with_fr)
plot(net.bg, layout=l)
plot(net.bg, layout=l)

dev.off()

# By default, the coordinates of the plots are rescaled to the [-1,1] interval
# for both x and y. You can change that with the parameter "rescale=FALSE"
# and rescale your plot manually by multiplying the coordinates by a scalar.
# You can use norm_coords to normalize the plot with the boundaries you want.
# This way you can create more compact or spread out layout versions.

# Get the layout coordinates:
l <- layout_with_fr(net.bg)
# Normalize them so that they are in the -1, 1 interval:
l <- norm_coords(l, ymin=-1, ymax=1, xmin=-1, xmax=1)

par(mfrow=c(2,2), mar=c(0,0,0,0))
plot(net.bg, rescale=F, layout=l*0.4)
plot(net.bg, rescale=F, layout=l*0.8)
plot(net.bg, rescale=F, layout=l*1.2)
plot(net.bg, rescale=F, layout=l*1.6)

dev.off()

# Another popular force-directed algorithm that produces nice results for
# connected graphs is Kamada Kawai. Like Fruchterman Reingold, it attempts to 
# minimize the energy in a spring system.

l <- layout_with_kk(net.bg)
plot(net.bg, layout=l)

#The MDS (multidimensional scaling) algorithm tries to place nodes based on some
# measure of similarity or distance between them. More similar/less distant nodes are
# placed closer to each other. By default, the measure used is based on the shortest 
#paths between nodes in the network. That can be changed with the 'dist' parameter.
plot(net.bg, layout=layout_with_mds)

# The LGL algorithm is for large connected graphs. Here you can specify a root - 
# the node that will be placed in the middle of the layout.
plot(net.bg, layout=layout_with_lgl)


# By default, igraph uses a layout called 'layout_nicely' which selects
# an appropriate layout algorithm based on the properties of the graph. 

# Check out all available layouts in igraph:
?igraph::layout_

layouts <- grep("^layout_", ls("package:igraph"), value=TRUE)[-1] 
# Remove layouts that do not apply to our graph.
layouts <- layouts[!grepl("bipartite|merge|norm|sugiyama|tree", layouts)]

par(mfrow=c(3,3), mar=c(1,1,1,1))

for (layout in layouts) {
  print(layout)
  l <- do.call(layout, list(net)) 
  plot(net, edge.arrow.mode=0, layout=l, main=layout) }

dev.off()



# -------~~ Highlighting aspects of the network --------


plot(net)

# Notice that our network plot is still not too helpful.
# We can identify the type and size of nodes, but cannot see
# much about the structure since the links we're examining are so dense.
# One way to approach this is to see if we can sparsify the network.

hist(links$weight)
mean(links$weight)
sd(links$weight)

# There are more sophisticated ways to extract the key edges,
# but for the purposes of this exercise we'll only keep ones
# that have weight higher than the mean for the network.

# We can delete edges using delete_edges(net, edges)
# (or, by the way, add edges with add_edges(net, edges) )
cut.off <- mean(links$weight) 
net.sp <- delete_edges(net, E(net)[weight<cut.off])
plot(net.sp, layout=layout_with_kk) 


# Another way to think about this is to plot the two tie types 
# (hyperlinks and mentions) separately. We will do that in 
# section 5 of this tutorial: Plotting multiplex networks.


# We can also try to make the network map more useful by
# showing the communities within it.

# Community detection (by optimizing modularity over partitions):
clp <- cluster_optimal(net)
class(clp)
clp$membership

# Community detection returns an object of class "communities" 
# which igraph knows how to plot: 
plot(clp, net)
 
# We can also plot the communities without relying on their built-in plot:
V(net)$community <- clp$membership
colrs <- adjustcolor( c("gray50", "tomato", "gold", "yellowgreen"), alpha=.6)
plot(net, vertex.color=colrs[V(net)$community])



# -------~~ Highlighting specific nodes or links --------


# Sometimes we want to focus the visualization on a particular node
# or a group of nodes. Let's represent distance from the NYT:
# distances() calculates shortest parth from vertices in 'v' to ones in 'to'.
dist.from.NYT <- distances(net, v=V(net)[media=="NY Times"], to=V(net), weights=NA)

# Set colors to plot the distances:
oranges <- colorRampPalette(c("dark red", "gold"))
col <- oranges(max(dist.from.NYT)+1)
col <- col[dist.from.NYT+1]
 
plot(net, vertex.label=dist.from.NYT, 
     vertex.color=col, vertex.label.color="white")

# Or, a bit more readable:
plot(net, vertex.color=col, vertex.label=dist.from.NYT, edge.arrow.size=.6, 
     vertex.label.color="white", vertex.size=V(net)$size*1.6, edge.width=2,
     layout=norm_coords(layout_with_lgl(net))*1.4, rescale=F)


# We can also highlight paths between the nodes in the network.
# Say here between MSNBC and the New York Post:
news.path <- shortest_paths(net, 
                            from = V(net)[media=="MSNBC"], 
                            to  = V(net)[media=="New York Post"],
                            output = "both") # both path nodes and edges

# Generate edge color variable to plot the path:
ecol <- rep("gray80", ecount(net))
ecol[unlist(news.path$epath)] <- "orange"
# Generate edge width variable to plot the path:
ew <- rep(2, ecount(net))
ew[unlist(news.path$epath)] <- 4
# Generate node color variable to plot the path:
vcol <- rep("gray40", vcount(net))
vcol[unlist(news.path$vpath)] <- "gold"

plot(net, vertex.color=vcol, edge.color=ecol, 
     edge.width=ew, edge.arrow.mode=0)


# Highlight the edges going into or out of a vertex, for instance the WSJ.
# For a single node, use 'incident()', for multiple nodes use 'incident_edges()'
inc.edges <- incident(net, V(net)[media=="Wall Street Journal"], mode="all")

# Set colors to plot the selected edges.
ecol <- rep("gray80", ecount(net))
ecol[inc.edges] <- "orange"
vcol <- rep("grey40", vcount(net))
vcol[V(net)$media=="Wall Street Journal"] <- "gold"
plot(net, vertex.color=vcol, edge.color=ecol, edge.width=2)


# Or we can highlight the immediate neighbors of a vertex, say WSJ.
# The 'neighbors' function finds all nodes one step out from the focal actor.
# To find the neighbors for multiple nodes, use 'adjacent_vertices()'.
# To find node neighborhoods going more than one step out, use function 'ego()'
# with parameter 'order' set to the number of steps out to go from the focal node(s).

neigh.nodes <- neighbors(net, V(net)[media=="Wall Street Journal"], mode="out")

# Set colors to plot the neighbors:
vcol[neigh.nodes] <- "#ff9d00"
plot(net, vertex.color=vcol)
 

# Another way to draw attention to a group of nodes:
# (This is generally not recommended since, depending on layout, nodes
#  that are not 'marked' can accidentally get placed on top of the mark)
plot(net, mark.groups=c(1,4,5,8), mark.col="#C5E5E7", mark.border=NA)
# Mark multiple groups:
plot(net, mark.groups=list(c(1,4,5,8), c(15:17)), 
          mark.col=c("#C5E5E7","#ECD89A"), mark.border=NA)



# -------~~ Interactive plotting with 'tkplot' -------- 

# R and igraph offer interactive plotting capabilities
# (mostly helpful for small networks)

tkid <- tkplot(net) #tkid is the id of the tkplot

l <- tkplot.getcoords(tkid) # grab the coordinates from tkplot
plot(net, layout=l)



# -------~~ Other ways to represent a network -------- 

# One reminder that there are other ways to represent a network:

# Heatmap of the network matrix:
netm <- as_adjacency_matrix(net, attr="weight", sparse=F)
colnames(netm) <- V(net)$media
rownames(netm) <- V(net)$media

palf <- colorRampPalette(c("gold", "dark orange")) 

# The Rowv & Colv parameters turn dendrograms on and off
heatmap(netm[,17:1], Rowv = NA, Colv = NA, col = palf(20), 
        scale="none", margins=c(10,10) )

# Degree distribution
deg.dist <- degree_distribution(net, cumulative=T, mode="all")
plot( x=0:max(degree(net)), y=1-deg.dist, pch=19, cex=1.4, col="orange", 
      xlab="Degree", ylab="Cumulative Frequency")



# ================ 4. Plotting two-mode networks ================


head(nodes2)
head(links2)

net2
plot(net2)

# This time we will make nodes look different based on their type.
# Media outlets are blue squares, audience nodes are orange circles:
V(net2)$color <- c("steel blue", "orange")[V(net2)$type+1]
V(net2)$shape <- c("square", "circle")[V(net2)$type+1]

# Media outlets will have name labels, audience members will not:
V(net2)$label <- ""
V(net2)$label[V(net2)$type==F] <- nodes2$media[V(net2)$type==F] 
V(net2)$label.cex=.6
V(net2)$label.font=2

plot(net2, vertex.label.color="white", vertex.size=(2-V(net2)$type)*8) 

# igraph has a built-in bipartite layout, though it's not the most helpful:
plot(net2, vertex.label=NA, vertex.size=7, layout=layout_as_bipartite) 

 
# Using text as nodes:
par(mar=c(0,0,0,0))
plot(net2, vertex.shape="none", vertex.label=nodes2$media,
     vertex.label.color=V(net2)$color, vertex.label.font=2, 
     vertex.label.cex=.95, edge.color="gray70",  edge.width=2)

dev.off()


# Using images as nodes
# You will need the 'png' package to do this:

# install.packages("png")
library("png")
 
img.1 <- readPNG("./Data files/images/news.png")
img.2 <- readPNG("./Data files/images/user.png")

V(net2)$raster <- list(img.1, img.2)[V(net2)$type+1]

par(mar=c(3,3,3,3))

plot(net2, vertex.shape="raster", vertex.label=NA,
     vertex.size=16, vertex.size2=16, edge.width=2)

# By the way, you can also add any image you want to any plot.
# For example, many network graphs could be improved by a photo
# of a puppy carrying a basket full of kittens.
img.3 <- readPNG("./Data files/images/puppy.png")
rasterImage(img.3,  xleft=-1.7, xright=0, ybottom=-1.2, ytop=0)


# The numbers after the image are coordinates for the plot.
# The limits of your plotting area are given in par()$usr

dev.off()

detach("package:png") 

# We can also generate and plot bipartite projections for the two-mode network:
# (co-memberships are easy to calculate by multiplying the network matrix by
# its transposed matrix, or using igraph's bipartite.projection function)

net2.bp <- bipartite.projection(net2)

# We can calculate the projections manually as well:
#   as_incidence_matrix(net2)  %*% t(as_incidence_matrix(net2))
# t(as_incidence_matrix(net2)) %*%   as_incidence_matrix(net2)

par(mfrow=c(1,2))

plot(net2.bp$proj1, vertex.label.color="black", vertex.label.dist=2,
     vertex.label=nodes2$media[!is.na(nodes2$media.type)])

plot(net2.bp$proj2, vertex.label.color="black", vertex.label.dist=2,
     vertex.label=nodes2$media[ is.na(nodes2$media.type)])

dev.off()

# PSA: Remember to detach packages when you are done with them!
# You may run into problems if you have igraph and Statnet packages loaded together.
detach("package:igraph")



# ================ 5. Plotting multiplex networks ================


# In some cases, the networks we want to plot are multigraphs: 
# they can have multiple edges connecting the same two nodes. 
#
# A related concept, multiplex networks, contain multiple types of ties
# -- e.g. friendship, romantic, and work relationships between individuals. 

# In our example network, we also have two tie types: hyperlinks and mentions. 

# One thing we can do is plot each type of tie separately:

library("igraph")

E(net)$width <- 2
plot(net, edge.color=c("dark red", "slategrey")[(E(net)$type=="hyperlink")+1],
      vertex.color="gray40", layout=layout_in_circle, edge.curved=.3)

# Another way to delete edges using the minus operator:  
net.m <- net - E(net)[E(net)$type=="hyperlink"]
net.h <- net - E(net)[E(net)$type=="mention"]

# Plot the two links separately:
par(mfrow=c(1,2))

plot(net.h, vertex.color="orange", layout=layout_with_fr, main="Tie: Hyperlink")
plot(net.m, vertex.color="lightsteelblue2", layout=layout_with_fr, main="Tie: Mention")

dev.off()

# Make sure the nodes stay in the same place in both plots:
par(mfrow=c(1,2),mar=c(1,1,4,1))

l <- layout_with_fr(net)
plot(net.h, vertex.color="orange", layout=l, main="Tie: Hyperlink")
plot(net.m, vertex.color="lightsteelblue2", layout=l, main="Tie: Mention")

dev.off()

# In our example network, we don't have node dyads connected by multiple
# types of connections (we never have both a 'hyperlink' and a 'mention'
# tie between the same two news outlets) -- however that could happen.

# One challenge in visualizing multiplex networks is that multiple 
# edges between the same two nodes may get plotted on top of each 
# other in a way that makes them impossible to distinguish. 

# For example, let us generate a simple multiplex network with
# two nodes and three ties between them:

multigtr <- graph( edges=c(1,2, 1,2, 1,2), n=2 )

l <- layout_with_kk(multigtr)

# Let's just plot the graph:
plot(multigtr, vertex.color="lightsteelblue", vertex.frame.color="white",
     vertex.size=40, vertex.shape="circle", vertex.label=NA,
     edge.color=c("gold", "tomato", "yellowgreen"), edge.width=10,
     edge.arrow.size=5, edge.curved=0.1, layout=l)

# Because all edges in the graph have the same curvature, they are drawn
# over each other so that we only see the last one. What we can do is 
# assign each edge a different curvature. One useful function in 'igraph'
# called curve_multiple() can help us here. For a graph G, curve.multiple(G)
# will generate a curvature for each edge that maximizes visibility.

plot(multigtr, vertex.color="lightsteelblue", vertex.frame.color="white", 
     vertex.size=40, vertex.shape="circle", vertex.label=NA,
     edge.color=c("gold", "tomato", "yellowgreen"), edge.width=10,
     edge.arrow.size=5, edge.curved=curve_multiple(multigtr), layout=l)


dev.off()

detach("package:igraph")

# ================ 6. Quick example using the 'network' package ================

# Plotting with the 'network' package is very similar to that with 'igraph' -
# although the notation is slightly different (a whole new set of parameter names!)
# Here is a quick example using the (by now very familiar) media network.

#Just in case we have forgotten this earlier:
dev.off()
detach("package:igraph")

# Load our main package:
library("network")

# Wait, what did our data look like?
head(links)
head(nodes)

# Convert the data into the network format used by the Statnet family.
# As in igraph, we can generate a 'network' object from an edgelist, 
# an adjacency matrix, or an incidence matrix. 
?edgeset.constructors

# Remember to set the 'ignore.eval' to F for weighted networks.
# We would need to set 'multile' to T if we wanted to allow multiplex edges.
net3 <- network(links, vertex.attr=nodes, matrix.type="edgelist", 
                loops=F, multiple=F, ignore.eval = F)
net3

# You can access the edges, vertices, and the network matrix using:
net3[,]
net3 %n% "net.name" <- "Media Network" #  network attribute
net3 %v% "media"    # Node attribute
net3 %e% "type"     # Edge attribute
 
net3 %v% "col" <- c("gray70", "tomato", "gold")[net3 %v% "media.type"]

# plot the network:
plot(net3, vertex.cex=(net3 %v% "audience.size")/7, vertex.col="col")

# For a full list of parameters that you can use in this plot, 
# check out ?plot.network.
?plot.network

# Note that - as in igraph - the plot returns the node position coordinates.
l <- plot(net3, vertex.cex=(net3 %v% "audience.size")/7, vertex.col="col")
plot(net3, vertex.cex=(net3 %v% "audience.size")/7, vertex.col="col", coord=l)


# The network package also offers the option to edit a plot interactively,
# by setting the parameter interactive=T

plot(net3, vertex.cex=(net3 %v% "audience.size")/7, vertex.col="col", interactive=T)


detach("package:network")




# ================ 7. Simple plot animations in R ================

# If you have already installed "ndtv", you should also have
# a package used by it called "animation".

# install.packages("animation")
library("animation")
library("igraph")

# In order for this to work, you need not only the R package, but also
# an additional software called ImageMagick from imagemagick.org 
# If you don't already have it, skip this part of the tutorial for now.

ani.options("convert") # Check that the package knows where to find ImageMagick
ani.options(convert="C:/Progra~1/ImageMagick-7.0.6-Q16/convert.exe") 

# You can use this technique to create various (not necessarily network-related)
# animations in R by generating multiple plots and combining them in an animated GIF.

l <- layout_with_lgl(net)

saveGIF( {  col <- rep("grey40", vcount(net))
            plot(net, vertex.color=col, layout=l)
            
            step.1 <- V(net)[media=="Wall Street Journal"]
            col[step.1] <- "#ff5100"
            plot(net, vertex.color=col, layout=l)
            
            step.2 <- unlist(neighborhood(net, 1, step.1, mode="out"))
            col[setdiff(step.2, step.1)] <- "#ff9d00"
            plot(net, vertex.color=col, layout=l) 
            
            step.3 <- unlist(neighborhood(net, 2, step.1, mode="out"))
            col[setdiff(step.3, step.2)] <- "#FFDD1F"
            plot(net, vertex.color=col, layout=l)  },
          interval = .8, movie.name="network_animation.gif" )

 detach("package:igraph")
 detach("package:animation")




# ================ 8. Interactive JavaScript networks ================


# There are a number of libraries like 'rcharts' and 'htmlwidgets' that can help you 
# export interactive web charts from R. We'll take a quick look at three packages that
# can export networks from R to JavaScript: : 'visNetwork' and 'threejs', and 'networkD3'


# -------~~  Interactive networks with visNetwork --------

# install.packages("visNetwork")

library("visNetwork") 

head(nodes)
head(links)

# We can visualize the network right away - visNetwork() will accept 
# our node and link data frames (it needs node data with an 'id' column,
# and edge data with 'from' and 'to' columns).

visNetwork(nodes, links)

# We can set the height and width of the  visNetwork() window 
# with parameters 'height' and 'width', the back color with 'background',
# the title, subtitle, and footer with 'main', 'submain', and 'footer'

visNetwork(nodes, links, height="300px", width="100%", background="#eeefff",
           main="Network", submain="And what a great network it is!",
           footer= "Hyperlinks and mentions among media sources")

# Like 'igraph' did, 'visNetwork' allows us to set graphic properties 
# as node or edge attributes directly in the data or through a function.

# Check out the available options with:
?visNodes
?visEdges
 

# We'll start by adding new node and edge attributes to our dataframes. 
# We'll leave our original data alone and create two new data frames:
vis.nodes <- nodes
vis.links <- links

# The options for node shape include 'ellipse', 'circle', 
# 'database', 'box', 'text', 'image', 'circularImage', 'diamond', 
# 'dot', 'star', 'triangle', 'triangleDown', 'square', and 'icon'

vis.nodes$shape  <- "dot"  
vis.nodes$shadow <- TRUE # Nodes will drop shadow
vis.nodes$title  <- vis.nodes$media # Text on click
vis.nodes$label  <- vis.nodes$type.label # Node label
vis.nodes$size   <- vis.nodes$audience.size # Node size
vis.nodes$borderWidth <- 2 # Node border width
 
# We can set the color for several elements of the nodes:
# "background" changes the node color, "border" changes the frame color;
# "highlight" sets the color on click, "hover" sets the color on mouseover.

vis.nodes$color.background <- c("slategrey", "tomato", "gold")[nodes$media.type]
vis.nodes$color.border <- "black"
vis.nodes$color.highlight.background <- "orange"
vis.nodes$color.highlight.border <- "darkred"

visNetwork(vis.nodes, vis.links)

# Below we change some of the visual properties of the edges:

vis.links$width <- 1+links$weight/8 # line width
vis.links$color <- "gray"    # line color  
vis.links$arrows <- "middle" # arrows: 'from', 'to', or 'middle'
vis.links$smooth <- FALSE    # should the edges be curved?
vis.links$shadow <- FALSE    # edge shadow

visNetwork(vis.nodes, vis.links)

# Remove the arrows and set the edge width to 1:
vis.links$arrows <- "" 
vis.links$width  <- 1
visnet <- visNetwork(vis.nodes, vis.links)
visnet

# We can also set the visualization options directly with visNodes() and visEdges()
visnet2 <- visNetwork(nodes, links)
visnet2 <- visNodes(visnet2, shape = "square", shadow = TRUE, 
                    color=list(background="gray", highlight="orange", border="black"))
visnet2 <- visEdges(visnet2, color=list(color="black", highlight = "orange"),
                    smooth = FALSE, width=2, dashes= TRUE, arrows = 'middle' ) 
visnet2


# 'visNetwork' offers a number of options, including highlighting the neighbors
#  of a selected node, or adding a drop-down menu to select a subset of nodes. 
# The subset is based on a column from our data - here the type label. 
visOptions(visnet, highlightNearest = TRUE, selectedBy = "type.label")
 

# 'visNetwork' can also work with predefined groups of nodes.
# Visual characteristics for each group can be set with visGroups().
nodes$group <- nodes$type.label 
visnet3 <- visNetwork(nodes, links)
visnet3 <- visGroups(visnet3, groupname = "Newspaper", shape = "square",
                     color = list(background = "gray", border="black"))
visnet3 <- visGroups(visnet3, groupname = "TV", shape = "dot",       
                     color = list(background = "tomato", border="black"))
visnet3 <- visGroups(visnet3, groupname = "Online", shape = "diamond",   
                     color = list(background = "orange", border="black"))
visLegend(visnet3, main="Legend", position="right", ncol=1) 


# For more information, check out:
?visOptions # available options 
?visLayout  # available layouts
?visGroups  # using node groups
?visLegend  # adding a legend
 
detach("package:visNetwork")



# -------~~ Interactive networks with threejs --------

# Another package exporting networks from R to a js library is 'threejs'
# The nice thing about it is that it can read igraph objects.

# install.packages("threejs")

# If you get errors or warnings using this library and the latest R version,
# try installing the development version of the 'htmlwidgets' package
# devtools::install_github("ramnathv/htmlwidgets")


library("threejs")
library("htmlwidgets")
library("igraph")
 
# The main network plotting function - graphjs() will take an igraph object.
# We could use our initial 'net' object with a slight modification - we will
# delete its graph layout and let 'threejs' generate its own layout.
# (We cheated a bit by assigning a function to the layout attribute above 
# rather than giving it a table of node coordinates. This is fine by 'igraph',
# but 'threejs' will not let us do it.
 
net.js <- net
graph_attr(net.js, "layout") <- NULL

# Note that RStudio for Windows may not render the 'threejs' graphics properly.
# We will save the output in an HTML file and open it in a browser.
# Some of the parameters that we can add include 'main' for the plot title;
# 'curvature' for the edge curvature; 'bg' for background color; 
# 'showLabels' to set labels to visible (TRUE) or not (FALSE); 
# 'attraction' and 'repulsion' to set how much nodes attract and repulse
# each other; 'opacity' for node transparency (0 to 1); 'stroke' to indicate
# whether nodes should be framed in a black circle (TRUE) or not (FALSE), etc.
# For the full list of parameters, check out ?graphjs

gjs <- graphjs(net.js, main="Network!", bg="gray10", showLabels=F, stroke=F, 
               curvature=0.1, attraction=0.9, repulsion=0.8, opacity=0.9)
print(gjs)
saveWidget(gjs, file="Media-Network-gjs.html")
browseURL("Media-Network-gjs.html")
 
# Once we open the resulting visualization in the browser, we can use the mouse to
# control it: scrollwheel to zoom in and out, the left mouse button to rotate 
# the network, and the right mouse button to pan. 

# We can also create simple animations with 'threejs' by using lists of
# layouts, vertex colors, and edge colors that will switch at each step.

gjs.an <- graphjs(net.js, bg="gray10", showLabels=F, stroke=F, 
                  layout=list(layout_randomly(net.js, dim=3),
                              layout_with_fr(net.js,  dim=3),
                              layout_with_drl(net.js, dim=3),  
                              layout_on_sphere(net.js)),
                  vertex.color=list(V(net.js)$color, "gray", "orange", V(net.js)$color),
                  main=list("Random Layout", "Fruchterman-Reingold", "DrL layout", "Sphere" ) )
print(gjs.an)
saveWidget(gjs.an, file="Media-Network-gjs-an.html")
browseURL("Media-Network-gjs-an.html")

# Another example is the 'Les Miserables' network included with the package:

data(LeMis)
lemis.net <- graphjs(LeMis, main="Les Miserables", showLabels=T)
print(lemis.net)
saveWidget(lemis.net, file="LeMis-Network-gjs.html")
browseURL("LeMis-Network-gjs.html")
 
detach("package:igraph")
detach("package:threejs")
detach("package:htmlwidgets")


# -------~~  Interactive Networks with networkD3 --------

# Another package using JavaScript to export networks: networkD3

# install.packages("networkD3")

library("networkD3") 
 
# d3ForceNetwork expects node IDs that are numeric and start from 0
# so we have to transform our character node IDs:

links.d3 <- data.frame(from=as.numeric(factor(links$from))-1, 
                 to=as.numeric(factor(links$to))-1 )

# The nodes need to be in the same order as the "source" column in links:
nodes.d3 <- cbind(idn=factor(nodes$media, levels=nodes$media), nodes) 

# The `Group` parameter is used to color the nodes.
# Nodesize is not (as you might think) the size of the node, but the
# number of the column in the node data that should be used for sizing.
# The `charge` parameter guides node repulsion (if negative) or 
# attraction (if positive).

forceNetwork(Links = links.d3, Nodes = nodes.d3, Source="from", Target="to",
               NodeID = "idn", Group = "type.label",linkWidth = 1,
               linkColour = "#afafaf", fontSize=12, zoom=T, legend=T,
               Nodesize=6, opacity = 1, charge=-600, 
               width = 600, height = 600)
                 

detach("package:networkD3")




# ================ 9. Interactive and dynamic networks with ndtv-d3 ================



# -------~~ Interactive network plots --------


# install.packages("ndtv", dependencies=T)

library("ndtv")

# You should not need additional software to produce web animations with 'ndtv' (below).
# If you want to save the animations as  video  files ( see ?saveVideo), you have to
# install a video converter called FFmpeg (ffmpg.org). To find out how to get the right 
# installation for your OS, check out ?install.ffmpeg  To use all available layouts, 
# you need to have Java installed on your machine.


# Remember net3, our original media network turned into a 'network' object:
net3 

# Let's create an interactive (but not yet dynamic!) visualization of net3.
# You will recognize a lot of the plotting parameters from 'network':
# Two new parameters set the tooltips (the popup labels you see when you 
# click on network elements); note that those can take html format.
# 'launchBrowser=T' will open file 'filename' in your default browser.

render.d3movie(net3, usearrows = F, displaylabels = F, bg="#111111", 
               vertex.border="#ffffff", vertex.col =  net3 %v% "col",
               vertex.cex = (net3 %v% "audience.size")/8, 
               edge.lwd = (net3 %e% "weight")/3, edge.col = '#55555599',
               vertex.tooltip = paste("<b>Name:</b>", (net3 %v% 'media') , "<br>",
                                    "<b>Type:</b>", (net3 %v% 'type.label')),
               edge.tooltip = paste("<b>Edge type:</b>", (net3 %e% 'type'), "<br>", 
                                  "<b>Edge weight:</b>", (net3 %e% 'weight' ) ),
               launchBrowser=T, filename="Media-Network.html" )  

    
# If you are going to embed this in a markdown document, 
# you would also need to add output.mode='inline' above.


# -------~~ Network evolution animations -------- 


# In order to work with the network animations in 'ndtv', 
# we need to understand the  dynamic network format used by 
# Statnet packages, implemented in 'networkDynamic'. It can 
# represent discrete or continuous longitudinal network structures. 

# Let's look at one of the example datasets included in the
# package, containing simulation data based on the network of
# business connections among Renaissance Florentine families:

data(short.stergm.sim)
short.stergm.sim 
head(as.data.frame(short.stergm.sim))

# Here, an edge starts from a node with ID in `tail` and goes to one with ID in `head`. 
# Edges exist from time point `onset` to time point `terminus`. 
# There may be multiple periods (*activity spells*) where an edge is present. 
# Each of those periods is recorded on a separate row in the data frame.


# With dynamic data, we can use 'network.extract()' to get a network  
# that only contains elements active at a given point/time interval.

# Plot the network ignoring time (all nodes & edges that were ever present):
plot(short.stergm.sim)  

# Plot the network at time 1 (at=1):
plot( network.extract(short.stergm.sim, at=1) )

# Plot nodes & edges active for the entire period (`rule=all`) from 1 to 5:
plot( network.extract(short.stergm.sim, onset=1, terminus=5, rule="all") )

#Plot nodes & edges active at any point (`rule=any`) between 1 and 10:
plot( network.extract(short.stergm.sim, onset=1, terminus=10, rule="any") ) 

# Let's make a quick d3 animation from the example network:
render.d3movie(short.stergm.sim,displaylabels=TRUE) 


# Next, we will create and animate our own dynamic network.

# Dynamic network object can be generated in a number of ways: from 
# a set of networks/matrices representing different time points, or from
# data frames/matrices with node lists and edge lists indicating when each
# is active, or when they switch state. See ?networkDynamic for more information.

net3
plot(net3)

vs <- data.frame(onset=0, terminus=50, vertex.id=1:17)
es <- data.frame(onset=1:49, terminus=50, 
                 head=as.matrix(net3, matrix.type="edgelist")[,1],
                 tail=as.matrix(net3, matrix.type="edgelist")[,2])
head(vs)
head(es)

# Combine our base network with the edge and node longitudinal data:
net3.dyn <- networkDynamic(base.net=net3, edge.spells=es, vertex.spells=vs)


# Plot the network (all elements present at any time point): 
plot(net3.dyn, vertex.cex=(net3 %v% "audience.size")/7, vertex.col="col")


# Plot static images showing network evolution:


compute.animation(net3.dyn, animation.mode = "kamadakawai",
                  slice.par=list(start=0, end=49, interval=10, 
                         aggregate.dur=10, rule='any'))

# Show time evolution through static images at different time points:
 filmstrip(net3.dyn, displaylabels=F, mfrow=c(2, 3),
           slice.par=list(start=0, end=49, interval=10, 
                         aggregate.dur=10, rule='any'))
 
# We can pre-compute the animation coordinates (otherwise they get calculated when 
# you generate the animation). Here 'animation.mode' is the layout algorithm - 
# one of "kamadakawai", "MDSJ", "Graphviz"and "useAttribute" (user-generated).

# Here 'slice.par' is a list of parameters controlling how the network visualization 
# moves through time. The parameter 'interval' is the time step between layouts, 
# 'aggregate.dur' is the period shown in each layout, 'rule' is the rule for 
# displaying elements (e.g. 'any': active at any point during that period, 
# 'all': active during the entire period, etc.)
 

# Let's make an actual animation: 
 
compute.animation(net3.dyn, animation.mode = "kamadakawai",
                  slice.par=list(start=0, end=50, interval=1, 
                         aggregate.dur=1, rule='any'))


render.d3movie(net3.dyn, usearrows = F, 
               displaylabels = F, label=net3 %v% "media",
               bg="#ffffff", vertex.border="#333333",
               vertex.cex = degree(net3)/2,  
               vertex.col = net3.dyn %v% "col",
               edge.lwd = (net3.dyn %e% "weight")/3, 
               edge.col = '#55555599',
               vertex.tooltip = paste("<b>Name:</b>", (net3.dyn %v% "media") , "<br>",
                                    "<b>Type:</b>", (net3.dyn %v% "type.label") ),
               edge.tooltip = paste("<b>Edge type:</b>", (net3.dyn %e% "type"), "<br>", 
                                  "<b>Edge weight:</b>", (net3.dyn %e% "weight" ) ),
               launchBrowser=T, filename="Media-Network-Dynamic.html",
               render.par=list(tween.frames = 30, show.time = F),
               plot.par=list(mar=c(0,0,0,0)) )


# In addition to dynamic nodes and edges, 'ndtv' takes dynamic attributes.
# We could have added those to the 'es' and 'vs' data frames above.
# However, the plotting function can also evaluate parameters
# and generate dynamic arguments on the fly. For example,
# function(slice) { do some calculations with slice } will perform operations
# on the current time slice network, letting us change parameters dynamically.

# See the node size below:

compute.animation(net3.dyn, animation.mode = "kamadakawai",
                  slice.par=list(start=0, end=50, interval=4, 
                         aggregate.dur=1, rule='any'))


render.d3movie(net3.dyn, usearrows = F, 
               displaylabels = F, label=net3 %v% "media",
               bg="#000000", vertex.border="#dddddd",
               vertex.cex = function(slice){ degree(slice)/2.5 },  
               vertex.col = net3.dyn %v% "col",
               edge.lwd = (net3.dyn %e% "weight")/3, 
               edge.col = '#55555599',
               vertex.tooltip = paste("<b>Name:</b>", (net3.dyn %v% "media") , "<br>",
                                    "<b>Type:</b>", (net3.dyn %v% "type.label") ),
               edge.tooltip = paste("<b>Edge type:</b>", (net3.dyn %e% "type"), "<br>", 
                                  "<b>Edge weight:</b>", (net3.dyn %e% "weight" ) ),
               launchBrowser=T, filename="Media-Network-even-more-Dynamic.html",
               render.par=list(tween.frames = 25, show.time = F) )


detach("package:ndtv")
detach("package:sna")
detach("package:networkDynamic")
detach("package:network")


# ================ 10. Plotting networks on a geographic map ================


# The example below plots a network on a map using base R and mapping libraries.

# Note that for those familiar with it, the package 'ggplot2' may provide 
# a more flexible way of doing this. Things there work similarly to below,
# but you would use borders() to plot the map and geom_path() for the edges.


rm(list = ls()) # clear the workspace 


# In order to plot on a map, we'll need two additional packages.
# If you do not already have them, install those now:
# install.packages("maps")
# install.packages("geosphere")

library("maps")
library("geosphere")

# Package 'maps' has built-in maps it can plot for you. For example:
# ('col' is map fill, 'border' is  border color, 'bg' is  background color)
par(mfrow = c(2,2))
map("usa", col="tomato",  border="gray10", fill=TRUE, bg="gray30")
map("state", col="orange",  border="gray10", fill=TRUE, bg="gray30")
map("county", col="palegreen",  border="gray10", fill=TRUE, bg="gray30")
map("world", col="skyblue",  border="gray10", fill=TRUE, bg="gray30")

dev.off()

# The data we will use contains US airports and flights among them. 
# The airport file includes info about latitude and longitude.
# If we did not have those, we could use 'geocode()' from 'ggmap'
# to get latitude and longitude for an address.

airports <- read.csv("./Data Files/Dataset3-Airlines-NODES.csv", header=TRUE) 
flights <- read.csv("./Data Files/Dataset3-Airlines-EDGES.csv", header=TRUE, as.is=TRUE)

head(flights)
head(airports)

# Select only large airports: ones with more than 10 connections in the data.
tab <- table(flights$Source)
big.id <- names(tab)[tab>10]
airports <- airports[airports$ID %in% big.id,]
flights  <- flights[flights$Source %in% big.id & 
                    flights$Target %in% big.id, ]


# Plot a map of the united states:
map("state", col="grey20", fill=TRUE, bg="black", lwd=0.1)

# Add a point on the map for each airport:
points(x=airports$longitude, y=airports$latitude, pch=19, 
       cex=airports$Visits/80, col="orange")

# Generate edge colors: lighter color means higher flight volume.
col.1 <- adjustcolor("orange red", alpha=0.4)
col.2 <- adjustcolor("orange", alpha=0.4)
edge.pal <- colorRampPalette(c(col.1, col.2), alpha = TRUE)
edge.col <- edge.pal(100)

# For each flight, we will generate the coordinates of an arc that connects
# its star and end point, using gcIntermediate() from package 'geosphere'.
# Then we will plot that arc over the map using lines().
for(i in 1:nrow(flights))  {
    node1 <- airports[airports$ID == flights[i,]$Source,]
    node2 <- airports[airports$ID == flights[i,]$Target,]
    
    arc <- gcIntermediate( c(node1[1,]$longitude, node1[1,]$latitude), 
                           c(node2[1,]$longitude, node2[1,]$latitude), 
                           n=1000, addStartEnd=TRUE )
    edge.ind <- round(100*flights[i,]$Freq / max(flights$Freq))
    
    lines(arc, col=edge.col[edge.ind], lwd=edge.ind/30)
}



# ================ |-------------| ================
 



```


#### Reference 

Network visualization with R: http://www.kateto.net/polnet2018




<!--chapter:end:01_plot_in_R.Rmd-->

# Correlation 



## Visulization of pair-wise correlation in R



## Correlation and p-values of all combinations of all rows of two matrices

### Prepare the data

```{r}
bac1 <- c(1,2,3,4,5)
bac2 <- c(2,3,4,5,1)
bac3 <- c(4,5,1,2,3)
bac4 <- c(5,1,2,3,4)
bac <- as.data.frame(cbind(bac1, bac2, bac3, bac4 ))
colnames(bac) <- c("station1", "station2", "station3", "station4")
rownames(bac) <- c("bac1", "bac2", "bac3", "bac4", "bac5")

fac1 <- c(1,2,3,4,5,6)
fac2 <- c(2,3,4,5,1,6)
fac3<- c(3,4,5,1,2,1)
fac4<- c(4,5,1,2,3, 6)
fac <- as.data.frame(cbind(fac1, fac2, fac3, fac4))
colnames(fac) <- c("station1", "station2", "station3", "station4")
rownames(fac) <- c("fac1", "fac2", "fac3", "fac4", "fac5", "fac6")

bac
fac
```




### Another way to do this is to use `corr.test`

You can just pass the full matrices to the `cor` function (or psych::corr.test)and it takes care of finding the correlation of the relevant columns.



```{r}
#install.packages("psych")
library(reshape2)
#cor.eff<-cor(t(fac), t(bac))
#coreff <- reshape2::melt(cor.eff)
pval <- psych::corr.test(t(fac), t(bac), adjust="fdr")
pval
cor_pval<- merge(melt(pval$r, value.name="cor"), melt(pval$p, value.name="p-value"), by=c("Var1", "Var2"))
head(cor_pval)

```


### Read data

```
data/cor_data_metablomics.txt         
data/cor_data_transcriptome.txt

```


### Calculate the correlation (Advance)


```{r}
tbac <- data.frame(t(bac))
tfac <- data.frame(t(fac))

f <- function (x, y) cor(x, y)
pval <- function(x, y) cor.test(x, y)$p.val

tab <- outer(tfac, tbac, Vectorize(f))
tabp<-outer(tfac, tbac, Vectorize(pval))

as.data.frame.table(tabp)
```

### Reference

Correlation/p values of all combinations of all rows of two matrices: https://stackoverflow.com/questions/41793219/correlation-p-values-of-all-combinations-of-all-rows-of-two-matrices



## Different ways to visulize correlation

### Display correlation using `chart.Correlation` in `PerformanceAnalytics`

```{r}
library(datasets)
data(iris)
library("PerformanceAnalytics")
chart.Correlation(iris[, 1:4], histogram=TRUE, pch=19)
```

```{r}
library(corrplot)
corrplot.mixed(cor(iris[, 1:4]), order="original", tl.col="black")
```

## Reference


Seven Easy Graphs to Visualize Correlation Matrices in R: http://jamesmarquezportfolio.com/correlation_matrices_in_r.html

Example 9.17: (much) better pairs plots: https://www.r-bloggers.com/example-9-17-much-better-pairs-plots/


Introduction to Correlation: https://rpubs.com/aaronsc32/linear-relationship-pearson-r-correlation


Spearman Rank Correlation: https://rstudio-pubs-static.s3.amazonaws.com/191093_4169c5282eb145a491a5b1924941a6ba.html


<!--chapter:end:02_correlation_in_R.Rmd-->

# (PART) Statistics {-}


# Standard deviation vs Standard error

## Reference

https://datascienceplus.com/standard-deviation-vs-standard-error/

<!--chapter:end:03_stand_dev_stand_err.Rmd-->

# T test

## Calculate t-statistic step by step in R


```{r}
#load dataset
library(reshape2)
data(tips)

```

```{r}
#create tip percent variable
tips$percent=tips$tip/tips$total_bill
```

```{r}
#Split dataset for ease
splits<-split(tips, tips$sex)

```

```{r}
#Save data sets
women<-splits[[1]] 
men<-splits[[2]]
```

```{r}
#variance by group sample size
var_women<-var(women$percent)/length(women$percent)
var_men<-var(men$percent)/length(men$percent)
```

```{r}
#Sum
total_variance<-var_women+var_men
```

```{r}
#Squre Root
sqrt_variance<-sqrt(total_variance)

#Group means by pooled variances
(mean(women$percent)-mean(men$percent))/sqrt_variance
#T.test
t.test(percent~sex, data=tips)
```


### 

Your data set is probably not the best illustrative example in terms of normality assumption... but anyway, here is some quick R code to reproduce some of the calculation of t.test().

#### Equal variances

```{r}
head(tips)
t.test(percent ~ sex, data=tips, var.equal=TRUE)

x1 <- tips$percent[tips$sex == "Female"]
x2 <- tips$percent[tips$sex == "Male"]
n1 <- length(x1)
n2 <- length(x2)

var.pooled <- weighted.mean(x=c(var(x1), var(x2)), w=c(n1 - 1, n2 - 1))

t <- (mean(x1) - mean(x2)) / sqrt(var.pooled / n1 + var.pooled / n2)
t
df <- n1 + n2 - 2
df
```




#### Unequal variance

```{r}
t.test(percent ~ sex, data=tips, var.equal=FALSE)
# Welch Two Sample t-test
# 
# data:  percent by sex
# t = 1.1433, df = 206.759, p-value = 0.2542
# alternative hypothesis: true difference in means is not equal to 0
# 95 percent confidence interval:
#   -0.006404119  0.024084498
# sample estimates:
#   mean in group Female   mean in group Male 
# 0.1664907            0.1576505 

x1 <- tips$percent[tips$sex == "Female"]
x2 <- tips$percent[tips$sex == "Male"]
n1 <- length(x1)
n2 <- length(x2)

t <- (mean(x1) - mean(x2)) / sqrt(var(x1) / n1 + var(x2) / n2)
t
# [1] 1.143277
df.num <- (var(x1) / n1 + var(x2) / n2)^2
df.denom <- var(x1)^2 / (n1^2 * (n1 - 1)) + var(x2)^2 / (n2^2 * (n2 - 1))
df <- df.num / df.denom
df
# [1] 206.7587
```


## Calculate the standard deviation in R

$variance = \frac{sum((x-mean(x))^2)}{(length(x)-1)}$


```{r}
a <- c(179,160,136,227)
sd(a)
sqrt(sum((a-mean(a))^2/(length(a)-1)))
```


## Reference


Calculate t-statistic step by step in R: https://stats.stackexchange.com/questions/141593/calculate-t-statistic-step-by-step-in-r

<!--chapter:end:04_ttest.Rmd-->


# Linear model


While the assumtions of a linear model are never perfectly met, we must still check if they are reasonable assumtions to work with. 



## 



```{r}
## Data from  https://github.com/thomas-haslwanter/statsintro_python/blob/master/ISP/Code_Quantlets/08_TestsMeanValues/anovaOneway/galton.csv
tab<-read.csv("data/galton.csv")
head(tab)
```

## Extract male data 

```{r}
tab_son = tab[tab$sex=="M", ]
head(tab_son)
plot(father~height, data=tab_son)
```






## Reference

How to apply Linear Regression in R: https://datascienceplus.com/how-to-apply-linear-regression-in-r/

R for Data Science: https://r4ds.had.co.nz/model-basics.html#a-simple-model

<!--chapter:end:05_linear_model.Rmd-->

# MRG

Sir Francis Galton (1822–1911) was an English statistician. He founded many concepts in statistics, such as correlation, quartile, percentile and regression, that are still being used today.



## Read the data

Consider the data collected by Francis Galton in the 1880s, stored in a modern format in the `galton.csv` file. In this file, `heights` is the variable containing the child’s heights, while the `father`’s and `mother`’s height is contained in the variables father and mother.

The family variable is a numerical code identifying children in the same family; the number of kids in this family is in `nkids`.



```{r}
## Data from  https://github.com/thomas-haslwanter/statsintro_python/blob/master/ISP/Code_Quantlets/08_TestsMeanValues/anovaOneway/galton.csv
tab<-read.csv("data/galton.csv")
head(tab)
```

Check the number of rows and columns:

```{r}
dim(tab)
```

Covert the column of `sex` into numberic values:

```{r}
tab$sex=as.numeric(tab$sex) -1 
head(tab)
```
Remove the columns of `nkids`: 

```{r}
tab<-tab[, -c(6)]
head(tab)
```









<!--chapter:end:06_multi_linear_regression.Rmd-->


# Multiple linear regression

## 


Descriptions of the data: https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/airquality.html

Daily air quality measurements in New York, May to September 1973.



```{r}
data("airquality")
names(airquality)
```


```{r}
require(graphics)
pairs(airquality, panel = panel.smooth, main = "airquality data")
```

A formula of the form `y ~ x | a` indicates that plots of `y` versus `x` should be produced conditional on the variable `a`. A formula of the form `y ~ x| a * b` indicates that plots of `y` versus `x` should be produced conditional on the two variables `a` and `b`

```{r}

coplot(Ozone~Solar.R|Wind,  panel = panel.smooth, airquality)
```

```{r}
model2 = lm(Ozone~Solar.R*Wind, airquality)

```

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(model2)
par(mfrow=c(1,1)) # Change back to 1 x 1

```
1. Residuals vs Fitted

This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable and the pattern could show up in this plot if the model doesn’t capture the non-linear relationship. If you find equally spread residuals around a horizontal line without distinct patterns, that is a good indication you don’t have non-linear relationships.

2. Normal Q-Q

This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good if residuals are lined well on the straight dashed line.

3. Scale-Location

It’s also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.

4. Residuals vs Leverage

This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis (whatever outliers mean). Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don’t really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don’t get along with the trend in the majority of the cases.

Unlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of a dashed line, Cook’s distance. When cases are outside of the Cook’s distance (meaning they have high Cook’s distance scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.


```{r}
#install.packages("UsingR")
suppressWarnings(suppressMessages(library(UsingR)))
data(father.son)
names(father.son)
```

```{r}
model_fs = lm(fheight~sheight, father.son)

```

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(model_fs)

```


## 


```{r}

summary(model2)
```
```{r}
par(mfrow=c(1,2)) # Change back to 1 x 1
termplot(model2, partial.resid=TRUE, col.res = "royalblue")
```

```{r}
summary(airquality$Solar.R)
summary(airquality$Wind)
```


```{r}
Solar1 = mean(airquality$Solar.R, na.rm=T)
Solar2 = 100
Solar3 = 300

p1 = predict(model2, data.frame(Solar.R=Solar1, Wind=1:20))
p2 = predict(model2, data.frame(Solar.R=Solar2, Wind=1:20))
p3 = predict(model2, data.frame(Solar.R=Solar3, Wind=1:20))
```

```{r}
p1
```

```{r}
plot(Ozone~Wind, airquality, col="grey")
lines(p1, col="red")
lines(p2, col="blue")
lines(p3, col="darkgreen")
```

<!--chapter:end:07_multi_reg_with_an_interaction_term.Rmd-->

# Bayesian newwork in R

Bayesian networks (BNs) are a type of graphical model that encode the conditional probability between different learning variables in a directed acyclic graph. There are benefits to using BNs compared to other unsupervised machine learning techniques. A few of these benefits are:

1. It is easy to exploit expert knowledge in BN models. 

2. BN models have been found to be very robust in the sense of noisy data, missing data and sparse data. 

3. Unlike many machine learning models (including Artificial Neural Network), which usually appear as a “black box,” all the parameters in BNs have an understandable semantic interpretation. 

```{r}
#install.packages("bnlearn")
```


## Practical experiments


### Analysis of the MAGIC population in Scutari et al., Genetics (2014)


#### Reading and preprocessing the data


```{r}
#install.packages(lme4)
#install.packages("bnlearn")
library(lme4)
library(bnlearn)
library(parallel)
# load the data.
## http://www.bnlearn.com/research/genetics14/prepd-magic.txt.xz
xzfd = xzfile("data/prepd-magic.txt.xz", open = "r")
magic = read.table(xzfd, header = TRUE,
          colClasses = c(rep("factor", 4), rep("numeric", 3164)))
close(xzfd)
```


```{r}
head(magic)
```
++++Afterwards, we substitute short labels to the marker names, which are quite long and make plotting problematic. In addition we identify which variables in the data are traits, which are markers, which contain variety IDs and pedigree information.

```{r}
names(magic)[12:14]
names(magic)[12:ncol(magic)] = paste("G", 12:ncol(magic) - 11, sep = "")
names(magic)[12:14]
```


Finally, we extract observations with incomplete data (in the traits and variety information, missing values in the markers have all been imputed).

```{r}
partial = magic[!complete.cases(magic), ]
margic = magic[complete.cases(magic),]
```

#### Performing cross-validation

The Bayesian networks model is fitted by the fit.the.model() function below, which takes the data and the type I error threshold alpha to use for structure learning as arguments.

```{r}
fit.the.model = function(data, alpha) {

   cpc = vector(length(traits), mode = "list")
   names(cpc) = traits

   # find the parents of each trait (may be genes or other traits).
   for (t in seq_along(traits)) {

     # BLUP away the family structure.
     m = lmer(as.formula(paste(traits[t], "~ (1|FUNNEL:PLANT)")), data = data)
     data[!is.na(data[, traits[t]]), traits[t]] =
       data[, traits[t]] - ranef(m)[[1]][paste(data$FUNNEL, data$PLANT, sep = ":"), 1]
     # find out the parents.
     cpc[[t]] = learn.nbr(data[, c(traits, genes)], node = traits[t], debug = FALSE,
                   method = "si.hiton.pc", test = "cor", alpha = alpha)

   }#FOR

   # merge the relevant variables to use for learning.
   nodes = unique(c(traits, unlist(cpc)))
   # yield has no children, and genes cannot depend on traits.
   blacklist = tiers2blacklist(list(nodes[!(nodes %in% traits)],
                 traits[traits != "YLD"], "YLD"))

   # build the Bayesian network.
   bn = hc(data[, nodes], blacklist = blacklist)

   return(bn)
}#FIT.THE.MODEL
```


### Reference

Analysis of the MAGIC population in Scutari et al., Genetics (2014): http://www.bnlearn.com/research/genetics14/.The contents of this page are licensed under the Creative Commons Attribution-Share Alike License. 




<!--chapter:end:08_bayesian_network.Rmd-->


<!--chapter:end:09_corr_test.Rmd-->

# The rationale for using negative binomial distribution to model read count in RNA-seq data


Several tools for DEG detection (including DESeq2 and eageR) model read counts as a negative binomial (NB) distribution. 

The nagative binomial distribtion, especitally in its alternative parameterization, can be used as an alternative to the Poisson distribution. It is espectially useful for discrete data over an unbounded positive range whose sample variance exceeds the sample mean. In such cases, the observations are overdispersed with respect to a Poisson distribution, for which the mean is equal to the variance. Hence a Poison distribution is not an appropriate model. Since the negative binomial distribution has one more parameter than the Poisson distribution, the second parameter can be used to adjust the variance independently of the mean. 

Discrete data over an unbounded positive range is one way of saying 'integer counts'. 

The negative binomial distribution also arises as a continuous mixture of Poisson distributions (i.e. a compound probability distribution) where the mixing distribution of the Poisson rate is a gamma distribution." The other important bit of information to know is that read counts for a sample in theory follow a Binomial(n,p) distribution, where n is the total number of reads and p is the probability of a read mapping to a specific gene. However, the binomial distribution is computationally inconvenient, since it involves computing factorials, and with millions of reads in each sample, the Poisson distribution (with lambda = n*p) is an excellent approximation to the Binomial while being far more mathematically tractable. So the Poisson noise quantifies the counting uncertainty, while the gamma distribution quantifies the variation in gene expression between replicates. The mixture of the two yields the negative binomial. See also section 2 of http://www.statsci.org/smyth/pubs/edgeRChapterPreprint.pdf


![](https://user-images.githubusercontent.com/20909751/53537996-f12bbb00-3ad9-11e9-97a9-6dc35195a50a.png)

If differential expresssion detection is our goal, we need to keep in mind that we model the distribution of counts for a given gene across replicates; we don't model the dsitrution of an individual samples. So for DE detection, we are not concerned with shape of distribution of counts or the whole organisms at all.  Poisson is a single parameter dist'n, with mean=variance. That assumption, which is really an approximation to the binomial, is suitable only for the variability associated with sampling the same DNA population (e.g. if you sequence multiple lanes of the same DNA, and assume no lane-specific effects, etc.). But if there is variation between your replicates (e.g. lab mice, people, etc.), the Poisson assumption will tend to underestimate the variance and any differences you observe (e.g. testing the null hypothesis that two groups have the same mean expression) will be overstated. This explaination is from the co-author of edgeR (https://www.biostars.org/p/6028/).




## Negative binomial in R



```{r}
set.seed(100)
# 1) n: number of number of observations.
# 2) size: target for number of successful trials, or dispersion parameter (the shape parameter of the gamma mixing distribution). Must be strictly positive, need not be integer.
# 3) prob: probability of success in each trial. 0 < prob <= 1.
obs_nb <- rnbinom(100000, 5, 0.5)
obs_pios <- rpois(10000, mean(obs_nb))
table(obs_nb)
table(obs_pios)
```

```{r}
hist(obs_nb, breaks = 1000)
```

```{r}
hist(obs_pios, breaks = 1000)
```

```{r}
var(obs_nb)
var(obs_pios)
```

Here we can clearly see that the variance (measure of variation.) in the simulated data with Poisson distribution () is higher than that with negative binomial distribtuon. 


```{r}
n = 10000
dist_nb = rep(NA, n)
for(i in 1:n){
    obs_nb_tem <- rnbinom(3, 5, 0.5)
    dist_nb[i] = var(obs_nb_tem)/mean(obs_nb_tem)
}
hist(dist_nb)
```

```{r}
n = 10000
dist_pios = rep(NA, n)
for(i in 1:n){
    obs_pois_tem <- rpois(3, 5)
    dist_pios[i] = var(obs_pois_tem)/mean(obs_pois_tem)
}
hist(dist_pios)
```
```{r}
hist(dist_nb - dist_pios, breaks=100, col = "lightblue", border = NA)
abline(v=0, col="red", lty="dashed")
```

### Real data

```{r}
read_count = read.table("data/yeast_EV_DNMT3B_count.tab", sep="\t", head=T)
read_count_sub = read_count[, c(4:6)]
head(read_count_sub)

mean_var_diff <- apply(read_count_sub, 1, var) -  apply(read_count_sub, 1, mean) 
summary(mean_var_diff)
hist(mean_var_diff,breaks=100000,xlim=c(-50000,100000))

```


## Reference

What's the rationale for using the negative binomial distribution to model read: https://support.bioconductor.org/p/84832/

Why do we use the negative binomial distribution for analysing RNAseq data?: http://bridgeslab.sph.umich.edu/posts/why-do-we-use-the-negative-binomial-distribution-for-rnaseq


R统计学(06): 负二项分布: https://mp.weixin.qq.com/s/QBkL8_cW6Lsm5U56SUmUoQ



## Batch effect 

Batch effects are sub-groups of measurements that have qualitatively different behaviour across conditions and are unrelated to the biological or scientific variables in a study. For example, batch effects may occur if a subset of experiments was run on Monday and another set on Tuesday, if two technicians were responsible for different subsets of the experiments or if two different lots of reagents, chips or instruments were used. These effects are not exclusive to high-throughput biology and genomics research1, and batch effects also affect low-dimensional molecular measurements, such as northern blots and quantitative PCR. 

batch effects occur because measurements are affected by laboratory conditions, reagent lots and personnel differences.


1.1 Data simulation
We downloaded an RNA-seq dataset from online for Zebrafish. This is a standard count data from RNA-seq experiment. These data are available as part of the zebrafishRNASeq Bioconductor package. (https://bioconductor.org/packages/release/data/experiment/html/zebrafishRNASeq.html) We first filter low expressed genes and only keep genes with more than 5 counts in at least two samples, and then check the dispersion of counts data by plotting the variance against the mean of the zebrafish dataset. Based on the higher dispersion usually existing in count data, we assume a zero inflated negative binomial distribution and estimate the parameters from the zebrafish data. We then use the R package of “ polyester” [24] to simulate 2 RNA-seq datasets by using the same dispersion parameters (mean and variance) from the zebrafish dataset, thus to mimic the real situation in a common RNA-seq experiment.

For the data simulation step, we simulate 10, 000 genes on 20 samples. These 20 samples are simulated on 2 batches, with 5 cases and 5 controls on each batch. So there will be 2 kinds of factors on each of the 10, 000 genes: group factor (case/control status) and batch factor (batch 1/2). The DEGs detected could be caused by either of these 2 factors.

For dataset 1, we simulate the group factor (case/control) on 0 gene (i.e., 0 gene will be detected as differentially expressed due to case/control status), and batch factor on 10, 000 genes (i.e., if any of these 10, 000 genes are detected as differentially expressed, that will only be caused by batch factor, not by sample phenotype of case/control status). In this case, the DEGs will be false positive signals because of the existence of batch effect and absence of case/control difference.

In second case, we generate a dataset 2 with random batch effects on case and control samples (i.e., the phenotype factors are not balanced between case and control samples in each batch and batch factor have different impact on case samples and control samples). For the genes, the group factors (case and control) are simulated on 3000 genes and batch factors are simulated on 5000 genes using R package of “ polyester” respectively. Finally, among all the 10, 000 genes, 1000 genes are with group factor only, 2000 genes are with both group and batch factors, 3000 genes are with batch factor only, and the rest 4000 genes have either group factor or batch factor. The 3000 genes with batch factor only are defined as control genes, so that we can test the impact of batch effects and the performance of each method in removing batch effects).




## Reference 

Simulation of batch effect data: 
https://www.biorxiv.org/content/biorxiv/suppl/2017/12/15/234344.DC1/234344-1.pdf 

Evaluation of Methods in Removing Batch Effects on RNA-seq Data: 
http://www.tran-med.com/article/2016/2411-2917-2-1-3.html#outline_anchor_4


<!--chapter:end:10_neg_binomial_count.Rmd-->

# Multiple test corretion


<!--chapter:end:11_multiple_test_correction.Rmd-->

# Working with missing values

## 

```{r}
## Some sample data
set.seed(0)
dat <- matrix(1:100, 10, 10)
dat[sample(1:100, 10)] <- NA
dat <- data.frame(dat)

```




## Summaryize the missing values


### Summarize the missing values using `mice` package

The `mice` package provides a nice function md.pattern() to get a better understanding of the pattern of missing data. 



```{r}
#install.packages("mice")
library(mice)
md.pattern(dat)
```

### Summarize the missing values using `VIM` packages

```{r}
#install.packages("VIM")
library(VIM)
aggr(dat, numbers = TRUE, prop = c(TRUE, FALSE))
aggr(t(dat), numbers = TRUE, prop = c(TRUE, FALSE))

```

```{r}
aggr_plot <- aggr(dat, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(dat), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```

```{r}
tdat = t(dat)
aggr_plot <- aggr(tdat, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(tdat), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```


## Delete columns/rows with more that x% missing 

```{r}
## Remove columns with more than 50% NA
dat_file = dat[, -which(colMeans(is.na(dat)) > 0.5)]

```

## Imputing the data

### Imputing the data with row-wise mean

```{r}
dat.imp = dat
k <- which(is.na(dat.imp), arr.ind=TRUE)
dat.imp[k] <- rowMeans(dat.imp, na.rm=TRUE)[k[,1]]
```


### Imputing the data with  row-wise mean using `mice`

```{r}
tempDat <- mice(as.matrix(dat), method = "mean")
summary(tempDat)
impDat<- complete(tempDat)
impDat
```


## Reference

Imputing Missing Data with R; MICE package: https://datascienceplus.com/imputing-missing-data-with-r-mice-package/

Delete columns/rows with more that x% missing : https://stackoverflow.com/questions/31848156/delete-columns-rows-with-more-that-x-missing


Tutorial on 5 Powerful R Packages used for imputing missing values
: https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/

<!--chapter:end:12_missing_values.Rmd-->

# Data wrangling


## 

```{r}
df[df == 0] <- NA
```


data<-replace(data.frame(lapply(data, as.character), stringsAsFactors = FALSE),
    !is.na(data), "1")


## Joining Data in R with dplyr

### Whats Covered

* Mutating joins
* Filtering joins and set operations
* Assembling data
* Advanced joining
* Case Study

Keys

* The Primary key needs to be unique in a table
* The foreign key in the second table can be duplicated
* second table will be matched to the primary table based on the primary key
* The primary key may be one, two or even more columns in the table


```{r}
#install.packages("dplyr")
library(dplyr)
```







## Reference

https://rpubs.com/williamsurles/293454

dplyr tutorial
: https://genomicsclass.github.io/book/pages/dplyr_tutorial.html


<!--chapter:end:13_intro_dplyr.Rmd-->

# (PART) Machine learning {-}


# PCA 

PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.

PCA is veru useful whne the varianle with the data set are highly correlated. Correlation indicates that there is redundancy in the data. Due to this redundancy, PCA can be used to reduce the original variable into smaller number of new variabeles, nameing principal components. 

## Read data

```{r}
```

```{r}
iris_tab <- read.csv("https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv")
```

## Head data

```{r}
head(iris_tab)
```

## Summary of the data

```{r}
summary(iris_tab)
```


## Check the distribution of the data

```{r}
hist(iris_tab$sepal_length)
```



## Eigendecomposition - Computing Eigenvectors and Eigenvalues

In linear algebra, an `eigenvector` is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.

The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the "core" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.

### Covariance Matrix

The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $\Sigma$, which is a $d \times d$ matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:

$\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{N}\left(  x_{ij}-\bar{x}_j \right)  \left( x_{ik}-\bar{x}_k \right).$

We can summarize the calculation of the covariance matrix via the following matrix equation:   

$\Sigma = \frac{1}{n-1} \left( (\mathbf{X} - \mathbf{\bar{x}})^T\;(\mathbf{X} - \mathbf{\bar{x}}) \right)$  
where $\mathbf{\bar{x}}$ is the mean vector 
$\mathbf{\bar{x}} = \sum\limits_{i=1}^n x_{i}.$  

The mean vector is a $d$-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.


```{r}
cov_mat<-cor(iris_tab[,1:4])
cov_mat
```

```{r}
#install.packages("corrplot")
library(corrplot)
corrplot(cov_mat)
```


## PCA with two variables


```{r}
new_data = as.data.frame(cbind(petal_length = iris_tab$petal_length, petal_width = iris_tab$petal_width))
head(new_data)
plot(new_data$petal_length, new_data$petal_width)
```



```{r}
#install.packages("FactoMineR")
library("FactoMineR")
pc_2var_res = PCA(new_data)
print(pc_2var_res)
```


```{r}
#install.packages("FactoInvestigate")
#library(FactoInvestigate)
#Investigate(pc_2var_res, document = "pdf_document")
```





Next, we perform an eigendecomposition on the covariance matrix:

```{r}
eigen(cov_mat)
```



## PCA on `wine` data

### Read the `wine` data

```{r}
wine <- read.csv("data/wine.csv", header=T)
knitr::kable(wine)
```

### Description of each column

```{r}
wine_desc <- read.table("data/wine_data_desc.tsv", sep="\t", header = T)
knitr::kable(wine_desc)
```


```{r}
library(corrplot)
dim(wine)
wine_x<-wine[, 2:14]
cor_matrix<-cor(wine_x)

# method = "square", ellipse, number, shade, color, pie
# type = upper, lower, full(default)
corrplot(cor_matrix, method="circle", type = "upper")
corrplot.mixed(cor_matrix)

```

### Example 

Depends on the goal of your analysis. Some common practices, some of which are mentioned in whuber's link:

Standardizing is usually done when the variables on which the PCA is performed are not measured on the same scale. Note that standardizing implies assigning equal importance to all variables.
If they are not measured on the same scale and you choose to work on the non standardized variables, it is often the case that each PC is dominated by a single variable and you just get a sort of ordering of the variables by their variance. (One of the loadings of each (early) component will be close to +1 or -1.)
The two methods often lead to different results, as you have experienced.

Intuitive example:

Suppose you have two variables: the height of a tree and the girth of the same tree. We will convert the volume to a factor: a tree will be high in volume if its volume is bigger than 20 cubic feet, and low in volume otherwise. We will use the trees dataset which comes preloaded in R

```{r}
data(trees)
tree.girth<-trees[,1]
tree.height<-trees[,2]
tree.vol<-as.factor(ifelse(trees[,3]>20,"high","low"))
```
Now suppose that the height was actually measured in miles instead of feet.

```{r}
tree.height<-tree.height/5280
tree<-cbind(tree.height,tree.girth)

#do the PCA
tree.pca<-princomp(tree)
summary(tree.pca)
```

The first component explains almost 100% of the variability in the data. The loadings:

```{r}
loadings(tree.pca)
```

```{r}
biplot(tree.pca,xlabs=tree.vol,col=c("grey","red"))
```

We see that trees high in volume tend to have a high tree girth, but the three height doesn't give any information on tree volume. This is likely wrong and the consequence of the two different unit measures.

We could use the same units, or we could standardize the variables. I expect both will lead to a more balanced picture of the variability. Of course in this case one can argue that the variables should have the same unit but not be standardized, which may be a valid argument, were it not that we are measuring two different things. (When we would be measuring the weight of the tree and the girth of the tree, the scale on which both should be measured is no longer very clear. In this case we have a clear argument to work on the standardized variables.)

```{r}
tree.height<-tree.height*5280
tree<-cbind(tree.height,tree.girth)

#do the PCA
tree.pca<-princomp(tree)
summary(tree.pca)

loadings(tree.pca)
biplot(tree.pca,xlabs=tree.vol,col=c("grey","red"))
```

We now see that trees which are tall and have a big girth, are high in volume (bottom left corner), compared to low girth and low height for low volume trees (upper right corner). This intuitively makes sense.

If one watches closely, however, we see that the contrast between high/low volume is strongest in the girth direction and not in the height direction. Let's see what happens when we standardize:

```{r}
tree<-scale(tree,center=F,scale=T)
tree.pca<-princomp(tree)
summary(tree.pca)
loadings(tree.pca)
biplot(tree.pca,xlabs=tree.vol,col=c("grey","red"))

```

Indeed, the girth now explains the majority of the difference in high and low volume trees! (The length of the arrow in the biplot is indicatory of the variance in the original variable.) So even if things are measured on the same scale, standardizing may be useful. Not standardizing may be recommended when we are for example comparing the length of different species of trees because this is exactly the same measurement.


## References

https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#about-iris

How to read PCA biplots and scree plots: https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/

Principal Component Analysis (PCA): 
https://learnche.org/pid/latent-variable-modelling/principal-component-analysis/index

Exploratory Multivariate Analysis by Example using R:
http://factominer.free.fr/course/index.html

https://github.com/gabi493/ADEI/blob/master/Husson%20F.%20et%20al.%20-%20Exploratory%20Multivariate%20Analysis%20by%20Example%20-%20Using%20R%20-%202011.pdf


Three Tips for Principal Component Analysis: 
https://www.theanalysisfactor.com/tips-principal-component-analysis/


Not normalizing data before PCA gives better explained variance ratio?: 
https://stats.stackexchange.com/questions/105592/not-normalizing-data-before-pca-gives-better-explained-variance-ratio

<!--chapter:end:14_PCA.Rmd-->

# Random forest



## Practical experiments


### Random forest for prediction of iris 


The `caret` package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

#### Required R packages

Required packages:

```
caret   
AppliedPredictiveModeling 
ellipse 
```

Attribute Information:

1. sepal length in cm 

2. sepal width in cm 

3. petal length in cm 

4. petal width in cm 

5. class: 

-- Iris Setosa 
-- Iris Versicolour 
-- Iris Virginica

```{r}
str(iris)
head(iris)
dim(iris)
```

#### Visulization

##### Scatterplot Matrix

```{r}
#install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
transparentTheme(trans = .4)
#install.packages("caret")
library(caret)
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

##### Scatterplot Matrix with Ellipses

```{r}
#install.packages("ellipse")
library(ellipse)
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "ellipse",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

##### Overlayed Density Plots

```{r}
featurePlot(x = iris[, 1:4], 
            y = iris$Species,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(4, 1), 
            auto.key = list(columns = 3))
```

##### Box Plots

```{r}
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(4,1 ), 
            auto.key = list(columns = 2))
```
 
 
##### Machine leraning - Random forest

##### Model Train 

```{r}
set.seed(186)
# Data Splitting
train_index <- createDataPartition(iris$Species, p = 0.75, , times=1, list = FALSE)
train_set = iris[train_index, ]
test_set  = iris[-train_index, ]
fit_rf_cv <- train(Species ~ ., data=train_set, method='rf', metric = "Accuracy",
                   trControl=trainControl(method="cv",number=5)) 
fit_rf_cv
## Variance importance
rfVarImpcv = varImp(fit_rf_cv)
rfVarImpcv
```


##### Testing test data

```{r}
## predict
test_set$predict_rf <- predict(fit_rf_cv, test_set, "raw")
confusionMatrix(test_set$predict_rf, test_set$Species)
```



<!--chapter:end:15_ml_random_forest.Rmd-->

# QTL analysis



## Reference

STAT/BMI 877: Statistical Methods for Molecular Biology: https://www.biostat.wisc.edu/~kendzior/STAT877/

QTL Mapping and reproducible research: https://github.com/kbroman/Teaching_UWStatGen2019

Karl Broman: http://kbroman.org/pages/teaching.html


<!--chapter:end:16_QTLanalysis.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`

CSAMA 2017: Statistical Data Analysis for Genome-Scale Biology: https://bioconductor.org/help/course-materials/2017/CSAMA/

Bioconductor Courses & Conferences: https://bioconductor.org/help/course-materials/

Scalable Genomics with R and Bioconductor: https://arxiv.org/pdf/1409.2864v1.pdf

What They Forgot to Teach You About R: https://whattheyforgot.org/

Data & models versioning for ML projects, make them shareable and reproducible : 

<!--chapter:end:17_reference_resouce.Rmd-->

# Doing Bayesian statistics in R

R has tons of excellent packages for Bayesian data analysis.

CRAN Task View: Bayesian Inference http://cran.r-project.org/web/views/Bayesian.html

```{r eval=F}
library(R.utils)
library(rstan)
library(R2jags)
library(manipulate)
library(MCMCpack)
library(R2WinBUGS)
```

```
library(manipulate)

p <- seq(from=0.005, to=0.995, by=0.005)


##  requires RStudio. It seems that in Rstudio Clould. This could not be ran. 
manipulate(
  {plot(p, dbeta(p, alpha.hyper, beta.hyper),
        col="blue", lwd=2, type="l", las=1, bty="n",
        ylim=c(0, 8), ylab="density",
        main="Beta prior distribution")
    polygon(c(p, rev(p)), c(dbeta(p, alpha.hyper, beta.hyper),
                            rep(0, length(p))), col=rgb(0, 0, 1, 0.2), border=NA)},
  alpha.hyper=slider(0.1, 10, step=0.1, initial=1),
  beta.hyper=slider(0.1, 10, step=0.1, initial=1))
```
## Reference


https://github.com/dlinzer/BayesBARUG/blob/master/Linzer-BayesBARUG.R

<!--chapter:end:18_bayesian_modeling.Rmd-->

# Transformation of data


## Log transformation 

The log transformation is the most popular among the different types of transformations used to transform skewed dat to approximately conform to normality. If the original data follows a log-normal distribution or approximately so, then the log-transformed data follows a normal or near normal distribution.


We use log2 (and not log10) is that fold changes might happen on a smaller scale than the thousands/millions/billions that make log10 transformations useful in visualizations. For a long time, a two-fold change was the "gold standard" of a significant change from baseline. If FC was less than that, it might be due to experimental error. Thus, the log2 was used (instead of log10, or natural log, or some other base system).


### The rationale of using log2 transformation 

```{r}
dat <- read.table("data/yeast_DESeq2_DEG.tab", header = T)
```




### Reference

Log-transformation and its implications for data analysis: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4120293/

Dumb question about LogFC: https://www.reddit.com/r/labrats/comments/7odtki/dumb_question_about_logfc/

<!--chapter:end:19_data_transformation.Rmd-->

# Goodness of fit in R


As a data scientist, occasionally, you receive a dataset and you would like to know what is the generative distribution for that dataset. In this post, I aim to show how we can answer that question in R. To do that let’s make an arbitrary dataset that we sample from a Gamma distribution. To make the problem a little more interesting, let add Gaussian noise to simulate measurement noise:

```{r}
num_of_samples = 1000
x <- rgamma(num_of_samples, shape = 10, scale = 3)
x <- x + rnorm(length(x), mean=0, sd = .1)
```

Basically, the process of finding the right distribution for a set of data can be broken down into four steps:

1. Visualization. plot the histogram of data 

2. Guess what distribution would fit to the data the best 

3. Use some statistical test for goodness of fit 

4. Repeat 2 and 3 if measure of goodness is not satisfactory

The first task is fairly simple. In R, we can use hist to plot the histogram of a vector of data.

```{r}
p1 <- hist(x,breaks=50, include.lowest=FALSE, right=FALSE)
```

The plot above shows the histogram of the above dataset:

The second task is a little bit tricky. It is mainly based on your experience and your knowledge of statistical distribution. Since we created the dataset ourselves, it is easy (surprisingly!) to guess the distribution. Let assume we know that the distribution is a Gamma distribution with shape 10 and scale 3.

The third task is to do some statistical testing to see if data is actually driven from the parametric distribution. These tests are call Goodness of fit. There are three well-known and widely use goodness of fit tests that also have nice package in R.


1. Chi Square test
2. Kolmogorov–Smirnov test
3. Cramér–von Mises criterion

All of the above tests are for statistical null hypothesis testing. For goodness of fit we have the following hypothesis:

* H0 = The data is consistent with a specified reference distribution.

* H1 = The data is NOT consistent with a specified reference distribution



## Reference

Goodness of fit test in R: https://www.r-bloggers.com/goodness-of-fit-test-in-r/

<!--chapter:end:20_goodmess_of_fit.Rmd-->

# Overdispersion


Overdispersion describes the observation that variation is higher than would be expected.


## Recognising (and testing for) overdispersion

```{r}
library(lme4)
data(grouseticks)
summary(grouseticks)
```


## Refefernce

https://biometry.github.io/APES/LectureNotes/2016-JAGS/Overdispersion/OverdispersionJAGS.pdf

https://newonlinecourses.science.psu.edu/stat504/node/162/



<!--chapter:end:21_overdispersion.Rmd-->

# Package management


I use the following script to reinstall all packages my packages from my personal site library for R 3.4:

```
## get the 
installed <- rownames(installed.packages())
pkgs <- dir("~/R/x86_64-pc-linux-gnu-library/3.4")
new <- setdiff(pkgs, installed)
new
install.packages(new)
```

<!--chapter:end:22_package_manage.Rmd-->

# Permutation and combination

As a matter of fact, a permutation is an ordered combination. There are basically two types of permutations, with repetition (or replacement) and without repetition (without replacement).


```{r}
require(combinat)
permn(3)
combn(3, 2)
```

```{r}
length(permn(3))
dim(combn(3,2))[2]
```


## Reference

Combinations and Permutations: https://www.mathsisfun.com/combinatorics/combinations-permutations.html

Combinations and permutations in R: https://davetang.org/muse/2013/09/09/combinations-and-permutations-in-r/

<!--chapter:end:23_permutation_combinaiton.Rmd-->

# Partial least square analysis

## Prepare the data


```{r}
library(ropls)
data(sacurine)
names(sacurine)
```


We attach sacurine to the search path and display a summary of the content of the dataMatrix, sampleMetadata and variableMetadata with the strF Function of the ropls package (see also str):

```{r}
attach(sacurine)
strF(dataMatrix)
head(dataMatrix)
```


```{r}
sampleMetadata
```


```{r}
summary(sampleMetadata)
```

## Principal Component Analysis (PCA)

We perform a PCA on the dataMatrix matrix (samples as rows, variables as columns), with the opls method:

```{r}
sacurine.pca <- opls(dataMatrix)

```


```{r}
genderFc <- sampleMetadata[, "gender"]
plot(sacurine.pca, typeVc = "x-score",
parAsColFcVn = genderFc, parEllipsesL = TRUE)

```

## Partial least-squares: PLS and PLS-DA

For PLS (and OPLS), the Y response(s) must be provided to the opls method. Y can be either a numeric vector (respectively matrix) for single (respectively multiple) (O)PLS regression, or a character factor for (O)PLS-DA classification as in the following example with the gender qualitative response:

```{r}
sacurine.plsda <- opls(dataMatrix, genderFc)

```


```{r}
genderFc
```

```{r}
head(dataMatrix)
```
## Reference 

https://bioconductor.org/packages/release/bioc/vignettes/ropls/inst/doc/ropls-vignette.html#4_hands-on

<!--chapter:end:24_pls_da.Rmd-->

# Scale

scale, with default settings, will calculate the mean and standard deviation of the entire vector, then "scale" each element by those values by subtracting the mean and dividing by the sd. (If you use  scale(x, scale=FALSE), it will only subtract the mean but not divide by the std deviation.)




```{r}
   set.seed(1)
   x <- runif(7)

   # Manually scaling
   (x - mean(x)) / sd(x)

   scale(x)
```

The Scale() Function
The scale() function makes use of the following arguments.

x: a numeric object
center: if TRUE, the objects’ column means are subtracted from the values in those columns (ignoring NAs); if FALSE, centering is not performed
scale: if TRUE, the centered column values are divided by the column’s standard deviation (when center is also TRUE; otherwise, the root mean square is used); if FALSE, scaling is not performed
Centering Variables
Normally, to center a variable, you would subtract the mean of all data points from each individual data point. With scale(), this can be accomplished in one simple call.

scale() function centers and/or scales the columns of a numeric matrix.

Normally, to create z-scores (standardized scores) from a variable, you would subtract the mean of all data points from each individual data point, then divide those points by the standard deviation of all points. Again, this can be accomplished in one call using scale().

> #generate z-scores for variable A using the scale() function
> scale(A, center = TRUE, scale = TRUE)



scale(x, center = TRUE, scale = TRUE)

x: numeric matrix
center: either a logical value or a numeric vector of length equal to the number of columns of x
scale: either a logical value or a numeric vector of length equal to the number of columns of x

```{r}
x <- matrix(1:9,3,3)
scale(x)
x
```

## Reference

https://www.r-bloggers.com/r-tutorial-series-centering-variables-and-generating-z-scores-with-the-scale-function/

https://stackoverflow.com/questions/20256028/understanding-scale-in-r

<!--chapter:end:25_scale.Rmd-->

# Web scraping


## Download Content from NCBI Databases using `RISmed`

### Search author name

```{r}
#install the RISmed package
#install.packages("RISmed")
library(RISmed)
```

```{r}
#now let's look up this dude called Shaojun Xie
res <- EUtilsSummary('Shaojun Xie', type='esearch', db='pubmed')
 
summary(res)
 
#what are the PubMed ids for the Author Shaojun Xie?
QueryId(res)

 
#limit by date
res2 <- EUtilsSummary('Shaojun Xie', type='esearch', db='pubmed', mindate='2010', maxdate='2019')
 
summary(res2)

#three publications in 2012
QueryId(res2)

```



### Search keyword


```{r}
#first how many total articles containing retrotransposon
str_key = 'WGBS'
res3 <- EUtilsSummary(str_key, type='esearch', db='pubmed', mindate='2008', maxdate='2018')
 
summary(res3)

 
#if you only want the number of articles
QueryCount(res3)
 
#tally each year beginning at 1970
#In order not to overload the E-utility servers, NCBI recommends that users post no more than three
#URL requests per second and limit large jobs to either weekends or between 9:00 PM and 5:00 AM
#Eastern time during weekdays. Failure to comply with this policy may result in an IP address being
#blocked from accessing NCBI.
 
tally <- array()
x <- 1
for (i in 2016:2018){
  Sys.sleep(1)
  r <- EUtilsSummary(str_key, type='esearch', db='pubmed', mindate=i, maxdate=i)
  tally[x] <- QueryCount(r)
  x <- x + 1
}
 
names(tally) <- 2016:2018
max(tally)
barplot(tally, las=2, ylim=c(0,max(tally)), main="Number of PubMed articles")
```

<!--chapter:end:26_web_scraping.Rmd-->

# Linear mixed model in R


## Data from an Oats Field Trial

The Oats data frame has 72 rows and 4 columns.

This data frame contains the following columns:

* __Block__: an ordered factor with levels VI < V < III < IV < II < I

* __Variety__: a factor with levels Golden Rain Marvellous Victory

* __nitro__: a numeric vector

* __yield__: a numeric vector


```{r}
library(MASS)

library(summarytools)
dfSummary(oats)
```

```{r}
summary(oats)
```


## Data visulization

```{r}
library(ggplot2)
ggplot(oats, aes(x = V, y = Y)) + 
    geom_point()
```


```{r}
library(ggplot2)
ggplot(oats, aes(x = V, y = Y, color = N)) + 
    geom_point() + facet_wrap(~N, nrow = 1)
```

```{r}
library(ggplot2)
ggplot(oats, aes(x = N, y = Y, color = N)) + 
    geom_point() + facet_wrap(~V, nrow = 1)
```


```{r}
library(ggplot2)
ggplot(oats, aes(x = N, y = Y, color = N)) + 
    geom_boxplot() + 
    geom_point() + facet_wrap(~V, nrow = 1)
```


## Mixed linear model

```{r}
politeness=
read.csv("http://www.bodowinter.com/tutorial/politeness_data.csv")
head(politeness)

```


```{r}
data(Oats)
```


<!--chapter:end:linear_mixed_model.Rmd-->

