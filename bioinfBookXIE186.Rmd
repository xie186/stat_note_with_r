---
title: "Statistics notes for Bioinformatics in R"
author: "Shaojun Xie"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook: default
  bookdown::pdf_book: default
bibliography: ["reference.bib"]

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=6, fig.height=6) 

```

# Preface

This book includes the notes I have during the using of R as a Bioinformatician. 

<!--chapter:end:index.Rmd-->

# Basic systax

## Setup

### Install packages

### Install 

#### Getting help on functions and packages


## Data type

## Data structures



R has multiple data structures. If you are familiar with excel you can think of data structures as building blocks of a table and the table itself, and a table is similar to a sheet in excel. Most of the time you will deal with tabular data sets, you will manipulate them, take sub-sections of them. It is essential to know what are the common data structures in R and how they can be used. R deals with named data structures, this means you can give names to data structures and manipulate or operate on them using those names.

## Read from and write into files


### Read Random Rows from A Huge File

>> Given R data frames stored in the memory, sometimes it is beneficial to sample and examine the data in a large-size csv file before importing into the data frame. To the best of my knowledge, there is no off-shelf R function performing such data sampling with a relatively low computing cost. Therefore, I drafted two utility functions serving this particular purpose, one with the LaF library and the other with the reticulate library by leveraging the power of Python. While the first function is more efficient and samples 3 records out of 336,776 in about 100 milliseconds, the second one is more for fun and a showcase of the reticulate package.

```{r}
#install.packages("LaF")
library(LaF)
sample1 <- function(file, n) {
  lf <- laf_open(detect_dm_csv(file, sep = ",", header = TRUE, factor_fraction = -1))
  return(read_lines(lf, sample(1:nrow(lf), n)))
}

sample1("data/galton.csv", 3)
```


### Reference

Read Random Rows from A Huge CSV File: https://www.r-bloggers.com/read-random-rows-from-a-huge-csv-file/



## Functions


## Control structure






<!--chapter:end:00_R_intro.Rmd-->

# Plot in R 

## Read the data

Consider the data collected by Francis Galton in the 1880s, stored in a modern format in the `galton.csv` file. In this file, `heights` is the variable containing the child’s heights, while the `father`’s and `mother`’s height is contained in the variables father and mother.
The family variable is a numerical code identifying children in the same family; the number of kids in this family is in `nkids`.


```{r}
## Data from  https://github.com/thomas-haslwanter/statsintro_python/blob/master/ISP/Code_Quantlets/08_TestsMeanValues/anovaOneway/galton.csv
tab<-read.csv("data/galton.csv")
head(tab)
```



## Line plot

## Basic line plot

```{r}
plot(tab$father, tab$height)
```


## Color the dots based on the `sex` information

```{r}
col_info = ifelse(tab$sex=="M", "royalblue", "darkred")
plot(tab$father, tab$height, col=col_info)
```


```{r}
col_info = ifelse(tab$sex=="M", "royalblue", "darkred")
plot(tab$father, tab$height, col=col_info, xlim=c(60, 80), ylim=c(60, 80))
legend(76, 65,pch=c(19,19),col=c("royalblue", "darkred"),c("female", "male"),bty="o",cex=.8)

```



## 







```{r}
meth_lev=read.table("data/global_meth_lev.tsv", header=TRUE)
meth_lev
```



## Draw a circle in R 


```{r}
# initialize a plot
plot(c(-1, 1), c(-1, 1), type = "n")

# prepare "circle data"
radius <- 1
theta <- seq(0, 2 * pi, length = 200)

# draw the circle
lines(x = radius * cos(theta), y = radius * sin(theta))
```



<!--chapter:end:01_plot_in_R.Rmd-->


# Visulization of pair-wise correlation in R


```{r}
library(datasets)
data(iris)
head(iris)
summary(iris)
```




```{r}
library("PerformanceAnalytics")

chart.Correlation(iris[, 1:4], histogram=TRUE, pch=19)
```


## Reference


Seven Easy Graphs to Visualize Correlation Matrices in R: http://jamesmarquezportfolio.com/correlation_matrices_in_r.html

Example 9.17: (much) better pairs plots: https://www.r-bloggers.com/example-9-17-much-better-pairs-plots/

<!--chapter:end:02_correlation_in_R.Rmd-->

# Standard deviation vs Standard error

## Reference

https://datascienceplus.com/standard-deviation-vs-standard-error/

<!--chapter:end:03_stand_dev_stand_err.Rmd-->

# T test

## Calculate t-statistic step by step in R


```{r}
#load dataset
library(reshape2)
data(tips)

```

```{r}
#create tip percent variable
tips$percent=tips$tip/tips$total_bill
```

```{r}
#Split dataset for ease
splits<-split(tips, tips$sex)

```

```{r}
#Save data sets
women<-splits[[1]] 
men<-splits[[2]]
```

```{r}
#variance by group sample size
var_women<-var(women$percent)/length(women$percent)
var_men<-var(men$percent)/length(men$percent)
```

```{r}
#Sum
total_variance<-var_women+var_men
```

```{r}
#Squre Root
sqrt_variance<-sqrt(total_variance)

#Group means by pooled variances
(mean(women$percent)-mean(men$percent))/sqrt_variance
#T.test
t.test(percent~sex, data=tips)
```

<!--chapter:end:04_ttest.Rmd-->


# Linear model


While the assumtions of a linear model are never perfectly met, we must still check if they are reasonable assumtions to work with. 



## 



```{r}
## Data from  https://github.com/thomas-haslwanter/statsintro_python/blob/master/ISP/Code_Quantlets/08_TestsMeanValues/anovaOneway/galton.csv
tab<-read.csv("data/galton.csv")
head(tab)
```

## Extract male data 

```{r}
tab_son = tab[tab$sex=="M", ]
head(tab_son)
plot(father~height, data=tab_son)
```






## Reference

How to apply Linear Regression in R: https://datascienceplus.com/how-to-apply-linear-regression-in-r/

R for Data Science: https://r4ds.had.co.nz/model-basics.html#a-simple-model

<!--chapter:end:05_linear_model.Rmd-->

# MRG

Sir Francis Galton (1822–1911) was an English statistician. He founded many concepts in statistics, such as correlation, quartile, percentile and regression, that are still being used today.



## Read the data

Consider the data collected by Francis Galton in the 1880s, stored in a modern format in the `galton.csv` file. In this file, `heights` is the variable containing the child’s heights, while the `father`’s and `mother`’s height is contained in the variables father and mother.

The family variable is a numerical code identifying children in the same family; the number of kids in this family is in `nkids`.



```{r}
## Data from  https://github.com/thomas-haslwanter/statsintro_python/blob/master/ISP/Code_Quantlets/08_TestsMeanValues/anovaOneway/galton.csv
tab<-read.csv("data/galton.csv")
head(tab)
```

Check the number of rows and columns:

```{r}
dim(tab)
```

Covert the column of `sex` into numberic values:

```{r}
tab$sex=as.numeric(tab$sex) -1 
head(tab)
```
Remove the columns of `nkids`: 

```{r}
tab<-tab[, -c(6)]
head(tab)
```

<!--chapter:end:06_multi_linear_regression.Rmd-->


# Multiple linear regression

## 


Descriptions of the data: https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/airquality.html

Daily air quality measurements in New York, May to September 1973.



```{r}
data("airquality")
names(airquality)
```


```{r}
require(graphics)
pairs(airquality, panel = panel.smooth, main = "airquality data")
```

A formula of the form `y ~ x | a` indicates that plots of `y` versus `x` should be produced conditional on the variable `a`. A formula of the form `y ~ x| a * b` indicates that plots of `y` versus `x` should be produced conditional on the two variables `a` and `b`

```{r}

coplot(Ozone~Solar.R|Wind,  panel = panel.smooth, airquality)
```

```{r}
model2 = lm(Ozone~Solar.R*Wind, airquality)

```

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(model2)
par(mfrow=c(1,1)) # Change back to 1 x 1

```
1. Residuals vs Fitted

This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable and the pattern could show up in this plot if the model doesn’t capture the non-linear relationship. If you find equally spread residuals around a horizontal line without distinct patterns, that is a good indication you don’t have non-linear relationships.

2. Normal Q-Q

This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It’s good if residuals are lined well on the straight dashed line.

3. Scale-Location

It’s also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.

4. Residuals vs Leverage

This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis (whatever outliers mean). Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldn’t be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they don’t really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they don’t get along with the trend in the majority of the cases.

Unlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of a dashed line, Cook’s distance. When cases are outside of the Cook’s distance (meaning they have high Cook’s distance scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.


```{r}
#install.packages("UsingR")
suppressWarnings(suppressMessages(library(UsingR)))
data(father.son)
names(father.son)
```

```{r}
model_fs = lm(fheight~sheight, father.son)

```

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(model_fs)

```


## 


```{r}

summary(model2)
```
```{r}
par(mfrow=c(1,2)) # Change back to 1 x 1
termplot(model2, partial.resid=TRUE, col.res = "royalblue")
```

```{r}
summary(airquality$Solar.R)
summary(airquality$Wind)
```


```{r}
Solar1 = mean(airquality$Solar.R, na.rm=T)
Solar2 = 100
Solar3 = 300

p1 = predict(model2, data.frame(Solar.R=Solar1, Wind=1:20))
p2 = predict(model2, data.frame(Solar.R=Solar2, Wind=1:20))
p3 = predict(model2, data.frame(Solar.R=Solar3, Wind=1:20))
```

```{r}
p1
```

```{r}
plot(Ozone~Wind, airquality, col="grey")
lines(p1, col="red")
lines(p2, col="blue")
lines(p3, col="darkgreen")
```

<!--chapter:end:07_multiple_regresssion_with_an_interaction_term.Rmd-->

# Bayesian newwork in R

Bayesian networks (BNs) are a type of graphical model that encode the conditional probability between different learning variables in a directed acyclic graph. There are benefits to using BNs compared to other unsupervised machine learning techniques. A few of these benefits are:

1. It is easy to exploit expert knowledge in BN models. 

2. BN models have been found to be very robust in the sense of noisy data, missing data and sparse data. 

3. Unlike many machine learning models (including Artificial Neural Network), which usually appear as a “black box,” all the parameters in BNs have an understandable semantic interpretation. 

```{r}
#install.packages("bnlearn")
```


## Practical experiments


### Analysis of the MAGIC population in Scutari et al., Genetics (2014)


#### Reading and preprocessing the data


```{r}
#install.packages(lme4)
#install.packages("bnlearn")
library(lme4)
library(bnlearn)
library(parallel)
# load the data.
## http://www.bnlearn.com/research/genetics14/prepd-magic.txt.xz
xzfd = xzfile("data/prepd-magic.txt.xz", open = "r")
magic = read.table(xzfd, header = TRUE,
          colClasses = c(rep("factor", 4), rep("numeric", 3164)))
close(xzfd)
```


```{r}
head(magic)
```
++++Afterwards, we substitute short labels to the marker names, which are quite long and make plotting problematic. In addition we identify which variables in the data are traits, which are markers, which contain variety IDs and pedigree information.

```{r}
names(magic)[12:14]
names(magic)[12:ncol(magic)] = paste("G", 12:ncol(magic) - 11, sep = "")
names(magic)[12:14]
```


Finally, we extract observations with incomplete data (in the traits and variety information, missing values in the markers have all been imputed).

```{r}
partial = magic[!complete.cases(magic), ]
margic = magic[complete.cases(magic),]
```

#### Performing cross-validation

The Bayesian networks model is fitted by the fit.the.model() function below, which takes the data and the type I error threshold alpha to use for structure learning as arguments.

```{r}
fit.the.model = function(data, alpha) {

   cpc = vector(length(traits), mode = "list")
   names(cpc) = traits

   # find the parents of each trait (may be genes or other traits).
   for (t in seq_along(traits)) {

     # BLUP away the family structure.
     m = lmer(as.formula(paste(traits[t], "~ (1|FUNNEL:PLANT)")), data = data)
     data[!is.na(data[, traits[t]]), traits[t]] =
       data[, traits[t]] - ranef(m)[[1]][paste(data$FUNNEL, data$PLANT, sep = ":"), 1]
     # find out the parents.
     cpc[[t]] = learn.nbr(data[, c(traits, genes)], node = traits[t], debug = FALSE,
                   method = "si.hiton.pc", test = "cor", alpha = alpha)

   }#FOR

   # merge the relevant variables to use for learning.
   nodes = unique(c(traits, unlist(cpc)))
   # yield has no children, and genes cannot depend on traits.
   blacklist = tiers2blacklist(list(nodes[!(nodes %in% traits)],
                 traits[traits != "YLD"], "YLD"))

   # build the Bayesian network.
   bn = hc(data[, nodes], blacklist = blacklist)

   return(bn)
}#FIT.THE.MODEL
```


### Reference

Analysis of the MAGIC population in Scutari et al., Genetics (2014): http://www.bnlearn.com/research/genetics14/.The contents of this page are licensed under the Creative Commons Attribution-Share Alike License. 




<!--chapter:end:08_bayesian_network.Rmd-->

# Correlation test




## Reference

Introduction to Correlation: https://rpubs.com/aaronsc32/linear-relationship-pearson-r-correlation


Spearman Rank Correlation
: https://rstudio-pubs-static.s3.amazonaws.com/191093_4169c5282eb145a491a5b1924941a6ba.html

<!--chapter:end:09_correlation_test.Rmd-->

# The rationale for using negative binomial distribution to model read count in RNA-seq data


Several tools for DEG detection (including DESeq2 and eageR) model read counts as a negative binomial (NB) distribution. 

The nagative binomial distribtion, especitally in its alternative parameterization, can be used as an alternative to the Poisson distribution. It is espectially useful for discrete data over an unbounded positive range whose sample variance exceeds the sample mean. In such cases, the observations are overdispersed with respect to a Poisson distribution, for which the mean is equal to the variance. Hence a Poison distribution is not an appropriate model. Since the negative binomial distribution has one more parameter than the Poisson distribution, the second parameter can be used to adjust the variance independently of the mean. 

Discrete data over an unbounded positive range is one way of saying 'integer counts'. 

The negative binomial distribution also arises as a continuous mixture of Poisson distributions (i.e. a compound probability distribution) where the mixing distribution of the Poisson rate is a gamma distribution." The other important bit of information to know is that read counts for a sample in theory follow a Binomial(n,p) distribution, where n is the total number of reads and p is the probability of a read mapping to a specific gene. However, the binomial distribution is computationally inconvenient, since it involves computing factorials, and with millions of reads in each sample, the Poisson distribution (with lambda = n*p) is an excellent approximation to the Binomial while being far more mathematically tractable. So the Poisson noise quantifies the counting uncertainty, while the gamma distribution quantifies the variation in gene expression between replicates. The mixture of the two yields the negative binomial. See also section 2 of http://www.statsci.org/smyth/pubs/edgeRChapterPreprint.pdf


![](https://user-images.githubusercontent.com/20909751/53537996-f12bbb00-3ad9-11e9-97a9-6dc35195a50a.png)


## Negative binomial in R



```{r}
set.seed(100)
# 1) n: number of number of observations.
# 2) size: target for number of successful trials, or dispersion parameter (the shape parameter of the gamma mixing distribution). Must be strictly positive, need not be integer.
# 3) prob: probability of success in each trial. 0 < prob <= 1.
obs_nb <- rnbinom(100000, 5, 0.5)
obs_pios <- rpois(10000, mean(obs_nb))
table(obs_nb)
table(obs_pios)
```

```{r}
hist(obs_nb, breaks = 1000)
```

```{r}
hist(obs_pios, breaks = 1000)
```

```{r}
var(obs_nb)
var(obs_pios)
```

Here we can clearly see that the variance (measure of variation.) in the simulated data with Poisson distribution () is higher than that with negative binomial distribtuon. 



## Reference

What's the rationale for using the negative binomial distribution to model read: https://support.bioconductor.org/p/84832/

Why do we use the negative binomial distribution for analysing RNAseq data?: http://bridgeslab.sph.umich.edu/posts/why-do-we-use-the-negative-binomial-distribution-for-rnaseq


R统计学(06): 负二项分布: https://mp.weixin.qq.com/s/QBkL8_cW6Lsm5U56SUmUoQ

<!--chapter:end:10_neg_binomial_count.Rmd-->

# Working with missing values

## 

```{r}
## Some sample data
set.seed(0)
dat <- matrix(1:100, 10, 10)
dat[sample(1:100, 10)] <- NA
dat <- data.frame(dat)

```




## Summaryize the missing values


### Summarize the missing values using `mice` package

The `mice` package provides a nice function md.pattern() to get a better understanding of the pattern of missing data. 



```{r}
#install.packages("mice")
library(mice)
md.pattern(dat)
```

### Summarize the missing values using `VIM` packages

```{r}
#install.packages("VIM")
library(VIM)
aggr(dat, numbers = TRUE, prop = c(TRUE, FALSE))
aggr(t(dat), numbers = TRUE, prop = c(TRUE, FALSE))

```

```{r}
aggr_plot <- aggr(dat, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(dat), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```

```{r}
tdat = t(dat)
aggr_plot <- aggr(tdat, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(tdat), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```


## Delete columns/rows with more that x% missing 

```{r}
## Remove columns with more than 50% NA
dat_file = dat[, -which(colMeans(is.na(dat)) > 0.5)]

```

## Imputing the data

### Imputing the data with row-wise mean

```{r}
dat.imp = dat
k <- which(is.na(dat.imp), arr.ind=TRUE)
dat.imp[k] <- rowMeans(dat.imp, na.rm=TRUE)[k[,1]]
```


### Imputing the data with  row-wise mean using `mice`

```{r}
tempDat <- mice(as.matrix(dat), method = "mean")
summary(tempDat)
impDat<- complete(tempDat)
impDat
```


## Reference

Imputing Missing Data with R; MICE package: https://datascienceplus.com/imputing-missing-data-with-r-mice-package/

Delete columns/rows with more that x% missing : https://stackoverflow.com/questions/31848156/delete-columns-rows-with-more-that-x-missing


Tutorial on 5 Powerful R Packages used for imputing missing values
: https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/

<!--chapter:end:11_missing_values.Rmd-->

# Joining Data in R with dplyr

## Whats Covered

* Mutating joins
* Filtering joins and set operations
* Assembling data
* Advanced joining
* Case Study

Keys

* The Primary key needs to be unique in a table
* The foreign key in the second table can be duplicated
* second table will be matched to the primary table based on the primary key
* The primary key may be one, two or even more columns in the table


```{r}
#install.packages("dplyr")
library(dplyr)
```







## Reference

https://rpubs.com/williamsurles/293454

dplyr tutorial
: https://genomicsclass.github.io/book/pages/dplyr_tutorial.html


<!--chapter:end:12_intro_dplyr.Rmd-->

# PCA 

PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.

PCA is veru useful whne the varianle with the data set are highly correlated. Correlation indicates that there is redundancy in the data. Due to this redundancy, PCA can be used to reduce the original variable into smaller number of new variabeles, nameing principal components. 

## Read data

```{r}
```

```{r}
iris_tab <- read.csv("https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv")
```

## Head data

```{r}
head(iris_tab)
```

## Summary of the data

```{r}
summary(iris_tab)
```


## Check the distribution of the data

```{r}
hist(iris_tab$sepal_length)
```



## Eigendecomposition - Computing Eigenvectors and Eigenvalues

In linear algebra, an `eigenvector` is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.

The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the "core" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.

### Covariance Matrix

The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $\Sigma$, which is a $d \times d$ matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:

$\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{N}\left(  x_{ij}-\bar{x}_j \right)  \left( x_{ik}-\bar{x}_k \right).$

We can summarize the calculation of the covariance matrix via the following matrix equation:   

$\Sigma = \frac{1}{n-1} \left( (\mathbf{X} - \mathbf{\bar{x}})^T\;(\mathbf{X} - \mathbf{\bar{x}}) \right)$  
where $\mathbf{\bar{x}}$ is the mean vector 
$\mathbf{\bar{x}} = \sum\limits_{i=1}^n x_{i}.$  

The mean vector is a $d$-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.


```{r}
cov_mat<-cor(iris_tab[,1:4])
cov_mat
```

```{r}
#install.packages("corrplot")
library(corrplot)
corrplot(cov_mat)
```


## PCA with two variables


```{r}
new_data = as.data.frame(cbind(petal_length = iris_tab$petal_length, petal_width = iris_tab$petal_width))
head(new_data)
plot(new_data$petal_length, new_data$petal_width)
```



```{r}
#install.packages("FactoMineR")
library("FactoMineR")
pc_2var_res = PCA(new_data)
print(pc_2var_res)
```


```{r}
#install.packages("FactoInvestigate")
#library(FactoInvestigate)
#Investigate(pc_2var_res, document = "pdf_document")
```





Next, we perform an eigendecomposition on the covariance matrix:

```{r}
eigen(cov_mat)
```



## PCA on `wine` data

### Read the `wine` data

```{r}
wine <- read.csv("data/wine.csv", header=T)
knitr::kable(wine)
```

### Description of each column

```{r}
wine_desc <- read.table("data/wine_data_desc.tsv", sep="\t", header = T)
knitr::kable(wine_desc)
```


```{r}
library(corrplot)
dim(wine)
wine_x<-wine[, 2:14]
cor_matrix<-cor(wine_x)

# method = "square", ellipse, number, shade, color, pie
# type = upper, lower, full(default)
corrplot(cor_matrix, method="circle", type = "upper")
corrplot.mixed(cor_matrix)

```

### 


## References

https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#about-iris

How to read PCA biplots and scree plots: https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/

Principal Component Analysis (PCA): 
https://learnche.org/pid/latent-variable-modelling/principal-component-analysis/index

Exploratory Multivariate Analysis by Example using R:
http://factominer.free.fr/course/index.html

https://github.com/gabi493/ADEI/blob/master/Husson%20F.%20et%20al.%20-%20Exploratory%20Multivariate%20Analysis%20by%20Example%20-%20Using%20R%20-%202011.pdf


<!--chapter:end:13_PCA.Rmd-->

# Random forest



## Practical experiments


### Random forest for prediction of iris 


The `caret` package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

#### Required R packages

Required packages:

```
caret   
AppliedPredictiveModeling 
ellipse 
```

Attribute Information:

1. sepal length in cm 

2. sepal width in cm 

3. petal length in cm 

4. petal width in cm 

5. class: 

-- Iris Setosa 
-- Iris Versicolour 
-- Iris Virginica

```{r}
str(iris)
head(iris)
dim(iris)
```

#### Visulization

##### Scatterplot Matrix

```{r}
#install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
transparentTheme(trans = .4)
#install.packages("caret")
library(caret)
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

##### Scatterplot Matrix with Ellipses

```{r}
#install.packages("ellipse")
library(ellipse)
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "ellipse",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

##### Overlayed Density Plots

```{r}
featurePlot(x = iris[, 1:4], 
            y = iris$Species,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(4, 1), 
            auto.key = list(columns = 3))
```

##### Box Plots

```{r}
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(4,1 ), 
            auto.key = list(columns = 2))
```
 
 
##### Machine leraning - Random forest

##### Model Train 

```{r}
set.seed(186)
# Data Splitting
train_index <- createDataPartition(iris$Species, p = 0.75, , times=1, list = FALSE)
train_set = iris[train_index, ]
test_set  = iris[-train_index, ]
fit_rf_cv <- train(Species ~ ., data=train_set, method='rf', metric = "Accuracy",
                   trControl=trainControl(method="cv",number=5)) 
fit_rf_cv
## Variance importance
rfVarImpcv = varImp(fit_rf_cv)
rfVarImpcv
```


##### Testing test data

```{r}
## predict
test_set$predict_rf <- predict(fit_rf_cv, test_set, "raw")
confusionMatrix(test_set$predict_rf, test_set$Species)
```



<!--chapter:end:14_ml_random_forest.Rmd-->

# QTL analysis



## Reference

STAT/BMI 877: Statistical Methods for Molecular Biology: https://www.biostat.wisc.edu/~kendzior/STAT877/

QTL Mapping and reproducible research: https://github.com/kbroman/Teaching_UWStatGen2019

Karl Broman: http://kbroman.org/pages/teaching.html


<!--chapter:end:15_QTLanalysis.Rmd-->

`r if (knitr::is_html_output()) '# References {-}'`

CSAMA 2017: Statistical Data Analysis for Genome-Scale Biology: https://bioconductor.org/help/course-materials/2017/CSAMA/

Bioconductor Courses & Conferences: https://bioconductor.org/help/course-materials/

Scalable Genomics with R and Bioconductor: https://arxiv.org/pdf/1409.2864v1.pdf

<!--chapter:end:16_reference_resouce.Rmd-->

# Multiple test corretion


<!--chapter:end:multiple_test_correction.Rmd-->

