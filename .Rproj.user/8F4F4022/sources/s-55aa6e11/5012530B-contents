# PCA 

PCA reduces the dimensionality of a multivariate data to two or three principal components, that can be visualized graphically, with minimal loss of information.

PCA is veru useful whne the varianle with the data set are highly correlated. Correlation indicates that there is redundancy in the data. Due to this redundancy, PCA can be used to reduce the original variable into smaller number of new variabeles, nameing principal components. 

## Read data

```{r}
```

```{r}
iris_tab <- read.csv("https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv")
```

## Head data

```{r}
head(iris_tab)
```

## Summary of the data

```{r}
summary(iris_tab)
```


## Check the distribution of the data

```{r}
hist(iris_tab$sepal_length)
```



## Eigendecomposition - Computing Eigenvectors and Eigenvalues

In linear algebra, an `eigenvector` is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.

The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the "core" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.

### Covariance Matrix

The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $\Sigma$, which is a $d \times d$ matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:

$\sigma_{jk} = \frac{1}{n-1}\sum_{i=1}^{N}\left(  x_{ij}-\bar{x}_j \right)  \left( x_{ik}-\bar{x}_k \right).$

We can summarize the calculation of the covariance matrix via the following matrix equation:   

$\Sigma = \frac{1}{n-1} \left( (\mathbf{X} - \mathbf{\bar{x}})^T\;(\mathbf{X} - \mathbf{\bar{x}}) \right)$  
where $\mathbf{\bar{x}}$ is the mean vector 
$\mathbf{\bar{x}} = \sum\limits_{i=1}^n x_{i}.$  

The mean vector is a $d$-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.


```{r}
cov_mat<-cor(iris_tab[,1:4])
cov_mat
```

```{r}
#install.packages("corrplot")
library(corrplot)
corrplot(cov_mat)
```


## PCA with two variables


```{r}
new_data = as.data.frame(cbind(petal_length = iris_tab$petal_length, petal_width = iris_tab$petal_width))
head(new_data)
plot(new_data$petal_length, new_data$petal_width)
```



```{r}
#install.packages("FactoMineR")
library("FactoMineR")
pc_2var_res = PCA(new_data)
print(pc_2var_res)
```


```{r}
#install.packages("FactoInvestigate")
#library(FactoInvestigate)
#Investigate(pc_2var_res, document = "pdf_document")
```





Next, we perform an eigendecomposition on the covariance matrix:

```{r}
eigen(cov_mat)
```

## References

https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#about-iris

How to read PCA biplots and scree plots: https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/

Principal Component Analysis (PCA): 
https://learnche.org/pid/latent-variable-modelling/principal-component-analysis/index

Exploratory Multivariate Analysis by Example using R:
http://factominer.free.fr/course/index.html

https://github.com/gabi493/ADEI/blob/master/Husson%20F.%20et%20al.%20-%20Exploratory%20Multivariate%20Analysis%20by%20Example%20-%20Using%20R%20-%202011.pdf

